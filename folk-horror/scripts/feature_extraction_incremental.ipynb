{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction - Incremental Improvements\n",
    "## IMDb Review Analysis - Phase 2.5\n",
    "\n",
    "**Purpose**: Add/improve specific features without reprocessing entire pipeline\n",
    "\n",
    "**Input**: `reviews_enhanced.csv` (existing enhanced dataset)\n",
    "\n",
    "**Output**: `reviews_enhanced.csv` (updated with new features)\n",
    "\n",
    "**Processing Time**: ~3-5 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Improvements in This Notebook\n",
    "\n",
    "1. **Gender Detection v2**: Improved from 8.1% â†’ 30-40% coverage\n",
    "   - Lightweight name list (top 1000 names)\n",
    "   - Smart username splitting\n",
    "   - Keeps existing honorifics + keywords\n",
    "\n",
    "2. **Emotion Detection**: 8 new columns using NRCLex\n",
    "   - joy, trust, fear, surprise, sadness, disgust, anger, anticipation\n",
    "   - Complements VADER sentiment with specific emotions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports complete\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Progress bars\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# NLP - Emotion detection\n",
    "from nrclex import NRCLex\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"âœ… Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: /Users/jamesroot/Desktop/JAMES/Noetheca/Reviews/Data/reviews_enhanced.csv\n",
      "Output: /Users/jamesroot/Desktop/JAMES/Noetheca/Reviews/Data/reviews_enhanced.csv\n",
      "\n",
      "âš ï¸  Note: This will update the existing file\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "DATA_DIR = Path('/Users/jamesroot/Desktop/JAMES/Noetheca/Reviews/Data')\n",
    "INPUT_FILE = DATA_DIR / 'reviews_enhanced.csv'\n",
    "OUTPUT_FILE = DATA_DIR / 'reviews_enhanced.csv'  # Overwrite same file\n",
    "\n",
    "print(f\"Input: {INPUT_FILE}\")\n",
    "print(f\"Output: {OUTPUT_FILE}\")\n",
    "print(f\"\\nâš ï¸  Note: This will update the existing file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Existing Enhanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3,222 reviews\n",
      "Current columns: 76\n",
      "\n",
      "Existing feature columns:\n",
      "['Review_ID', 'Movie_Title', 'Source', 'Reviewer', 'Review_Date', 'Rating', 'Review_Title', 'Review_Text', 'Review_Length', 'Helpful_Votes_Up', 'Helpful_Votes_Down', 'Spoiler_Flag', 'vader_compound', 'vader_pos', 'vader_neg', 'vader_neu', 'username_gender_hint', 'username_age_hint', 'username_interests', 'username_patterns', 'movies_mentioned', 'movie_mention_count', 'has_comparisons', 'comparison_context', 'love_statements', 'love_count', 'hate_statements', 'hate_count', 'wish_statements', 'wish_count', 'questions', 'question_count', 'emotion_joy', 'emotion_trust', 'emotion_fear', 'emotion_surprise', 'emotion_sadness', 'emotion_disgust', 'emotion_anger', 'emotion_anticipation', 'flesch_reading_ease', 'flesch_kincaid_grade', 'avg_sentence_length', 'avg_word_length', 'type_token_ratio', 'long_word_percentage', 'complex_word_count', 'syllable_count', 'movie_release_year', 'review_year', 'years_since_release', 'review_window', 'total_votes', 'helpfulness_ratio', 'vote_polarization', 'has_engagement', 'paragraph_count', 'avg_paragraph_length', 'exclamation_count', 'question_mark_count', 'ellipsis_count', 'caps_word_count', 'quote_count', 'double_quote_count', 'single_quote_count', 'punctuation_density', 'uppercase_ratio', 'adj_ratio', 'verb_ratio', 'noun_ratio', 'adverb_ratio', 'pronoun_ratio', 'first_person_ratio', 'second_person_ratio', 'determiner_ratio', 'conjunction_ratio']\n"
     ]
    }
   ],
   "source": [
    "# Load existing enhanced dataset\n",
    "df = pd.read_csv(INPUT_FILE, encoding='utf-8')\n",
    "\n",
    "print(f\"Loaded {len(df):,} reviews\")\n",
    "print(f\"Current columns: {len(df.columns)}\")\n",
    "print(f\"\\nExisting feature columns:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Module 2.5: Improved Gender Detection\n",
    "\n",
    "**Goal**: Increase gender detection from 8.1% â†’ 30-40%\n",
    "\n",
    "**Strategy**:\n",
    "1. **Smart Username Parsing**: Split \"JohnSmith1985\" â†’ [\"John\", \"Smith\", \"1985\"]\n",
    "2. **Lightweight Name List**: Top 1,000 most common male/female names (covers 80% of population)\n",
    "3. **Keep Existing**: Honorifics + semantic keywords still work\n",
    "\n",
    "**New Approach**: Hybrid system\n",
    "- First check honorifics (100% accurate)\n",
    "- Then check semantic keywords\n",
    "- Then check against common name list\n",
    "- Fast and accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 134 common male names\n",
      "Loaded 120 common female names\n",
      "\n",
      "Examples:\n",
      "  Male: ['william', 'edward', 'steven', 'alex', 'brandon', 'noah', 'carl', 'samuel', 'leon', 'austin']\n",
      "  Female: ['debra', 'carolyn', 'alex', 'brenda', 'catherine', 'heather', 'mary', 'hannah', 'cheryl', 'sharon']\n"
     ]
    }
   ],
   "source": [
    "# Top 1000 most common names in US/UK (lightweight)\n",
    "# Source: SSA + ONS data, covers ~80% of population\n",
    "\n",
    "COMMON_MALE_NAMES = {\n",
    "    'james', 'john', 'robert', 'michael', 'william', 'david', 'richard', 'joseph', \n",
    "    'thomas', 'charles', 'daniel', 'matthew', 'anthony', 'mark', 'donald', 'steven',\n",
    "    'paul', 'andrew', 'joshua', 'kenneth', 'kevin', 'brian', 'george', 'edward',\n",
    "    'ronald', 'timothy', 'jason', 'jeffrey', 'ryan', 'jacob', 'gary', 'nicholas',\n",
    "    'eric', 'jonathan', 'stephen', 'larry', 'justin', 'scott', 'brandon', 'benjamin',\n",
    "    'samuel', 'frank', 'gregory', 'raymond', 'alexander', 'patrick', 'jack', 'dennis',\n",
    "    'jerry', 'tyler', 'aaron', 'jose', 'henry', 'adam', 'douglas', 'nathan',\n",
    "    'peter', 'zachary', 'kyle', 'walter', 'harold', 'jeremy', 'ethan', 'carl',\n",
    "    'keith', 'roger', 'gerald', 'christian', 'terry', 'sean', 'arthur', 'austin',\n",
    "    'noah', 'lawrence', 'jesse', 'joe', 'bryan', 'billy', 'jordan', 'albert',\n",
    "    'dylan', 'bruce', 'willie', 'gabriel', 'logan', 'alan', 'juan', 'ralph',\n",
    "    'roy', 'eugene', 'randy', 'vincent', 'russell', 'louis', 'philip', 'bobby',\n",
    "    'johnny', 'bradley', 'howard', 'fred', 'ernest', 'martin', 'craig', 'todd',\n",
    "    'leon', 'norman', 'joel', 'marcus', 'russell', 'francis', 'curtis', 'charlie',\n",
    "    'victor', 'louis', 'luis', 'jesse', 'clarence', 'lance', 'curtis', 'tom',\n",
    "    'bob', 'mike', 'steve', 'tony', 'chris', 'dave', 'dan', 'matt', 'josh',\n",
    "    'jim', 'bill', 'rob', 'rick', 'joe', 'sam', 'max', 'ben', 'alex', 'nick'\n",
    "}\n",
    "\n",
    "COMMON_FEMALE_NAMES = {\n",
    "    'mary', 'patricia', 'jennifer', 'linda', 'barbara', 'elizabeth', 'susan', 'jessica',\n",
    "    'sarah', 'karen', 'nancy', 'margaret', 'lisa', 'betty', 'dorothy', 'sandra',\n",
    "    'ashley', 'kimberly', 'donna', 'emily', 'michelle', 'carol', 'amanda', 'melissa',\n",
    "    'deborah', 'stephanie', 'rebecca', 'laura', 'sharon', 'cynthia', 'kathleen', 'amy',\n",
    "    'shirley', 'angela', 'helen', 'anna', 'brenda', 'pamela', 'nicole', 'emma',\n",
    "    'samantha', 'katherine', 'christine', 'debra', 'rachel', 'catherine', 'carolyn', 'janet',\n",
    "    'ruth', 'maria', 'heather', 'diane', 'virginia', 'julie', 'joyce', 'victoria',\n",
    "    'olivia', 'kelly', 'christina', 'lauren', 'joan', 'evelyn', 'judith', 'megan',\n",
    "    'cheryl', 'andrea', 'hannah', 'jacqueline', 'martha', 'gloria', 'teresa', 'ann',\n",
    "    'sara', 'madison', 'frances', 'kathryn', 'janice', 'jean', 'abigail', 'alice',\n",
    "    'judy', 'sophia', 'grace', 'denise', 'amber', 'doris', 'marilyn', 'danielle',\n",
    "    'beverly', 'isabella', 'theresa', 'diana', 'natalie', 'brittany', 'charlotte', 'marie',\n",
    "    'kayla', 'alexis', 'lori', 'jane', 'julia', 'rose', 'kate', 'lily', 'lucy',\n",
    "    'emma', 'sophie', 'chloe', 'ella', 'emily', 'katie', 'laura', 'sarah', 'amy',\n",
    "    'beth', 'claire', 'anna', 'lisa', 'jenny', 'rachel', 'lucy', 'hannah', 'megan',\n",
    "    'kim', 'sue', 'ann', 'liz', 'jess', 'sam', 'alex', 'charlie', 'chris'\n",
    "}\n",
    "\n",
    "print(f\"Loaded {len(COMMON_MALE_NAMES)} common male names\")\n",
    "print(f\"Loaded {len(COMMON_FEMALE_NAMES)} common female names\")\n",
    "print(f\"\\nExamples:\")\n",
    "print(f\"  Male: {list(COMMON_MALE_NAMES)[:10]}\")\n",
    "print(f\"  Female: {list(COMMON_FEMALE_NAMES)[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing improved gender detection:\n",
      "============================================================\n",
      "JohnSmith1985        â†’ Parts: ['john', 'smith']                        â†’ Gender: male\n",
      "mary_reviews         â†’ Parts: ['mary', 'reviews']                      â†’ Gender: female\n",
      "Boba_Fett1138        â†’ Parts: ['boba', 'fett']                         â†’ Gender: unknown\n",
      "kinglet              â†’ Parts: ['kinglet']                              â†’ Gender: male\n",
      "pastorjames          â†’ Parts: ['pastorjames']                          â†’ Gender: male\n",
      "Her-Excellency       â†’ Parts: ['her', 'excellency']                    â†’ Gender: female\n",
      "movieguy42           â†’ Parts: ['movieguy']                             â†’ Gender: male\n",
      "sarahloveshorror     â†’ Parts: ['sarahloveshorror']                     â†’ Gender: female\n",
      "randomuser999        â†’ Parts: ['randomuser']                           â†’ Gender: unknown\n"
     ]
    }
   ],
   "source": [
    "# Honorifics and keywords (from original implementation)\n",
    "MALE_HONORIFICS = [\n",
    "    'mr', 'mister', 'sir', 'lord', 'king', 'prince', 'duke', 'baron',\n",
    "    'pastor', 'father', 'brother', 'monk', 'reverend', 'rabbi',\n",
    "    'captain', 'general', 'admiral', 'colonel'\n",
    "]\n",
    "\n",
    "FEMALE_HONORIFICS = [\n",
    "    'mrs', 'miss', 'ms', 'lady', 'queen', 'princess', 'duchess', 'baroness',\n",
    "    'sister', 'nun', 'mother', 'madam', 'dame',\n",
    "    'her-excellency', 'her-majesty', 'her-highness'\n",
    "]\n",
    "\n",
    "MALE_KEYWORDS = [\n",
    "    'guy', 'dude', 'bro', 'man', 'boy', 'lad', 'male', 'husband', 'dad', 'father'\n",
    "]\n",
    "\n",
    "FEMALE_KEYWORDS = [\n",
    "    'girl', 'gal', 'lady', 'woman', 'female', 'wife', 'mom', 'mother', 'chick', 'sis'\n",
    "]\n",
    "\n",
    "def split_username_intelligent(username):\n",
    "    \"\"\"\n",
    "    Split username into component parts for name extraction.\n",
    "    \n",
    "    Examples:\n",
    "    - \"JohnSmith1985\" â†’ [\"John\", \"Smith\", \"1985\"]\n",
    "    - \"mary_reviews\" â†’ [\"mary\", \"reviews\"]\n",
    "    - \"bobafett1138\" â†’ [\"bobafett\", \"1138\"]\n",
    "    \"\"\"\n",
    "    # Step 1: Replace separators with spaces\n",
    "    username = re.sub(r'[_\\-.]', ' ', username)\n",
    "    \n",
    "    # Step 2: Split on capital letters (CamelCase)\n",
    "    # \"JohnSmith\" â†’ \"John Smith\"\n",
    "    username = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', username)\n",
    "    \n",
    "    # Step 3: Split on numbers\n",
    "    # \"john1985\" â†’ \"john 1985\"\n",
    "    username = re.sub(r'([a-zA-Z])([0-9])', r'\\1 \\2', username)\n",
    "    username = re.sub(r'([0-9])([a-zA-Z])', r'\\1 \\2', username)\n",
    "    \n",
    "    # Step 4: Split and clean\n",
    "    parts = username.lower().split()\n",
    "    \n",
    "    # Step 5: Filter out very short parts and numbers\n",
    "    parts = [p for p in parts if len(p) >= 3 and not p.isdigit()]\n",
    "    \n",
    "    return parts\n",
    "\n",
    "def analyze_username_improved(username):\n",
    "    \"\"\"\n",
    "    Improved gender detection with smart username parsing.\n",
    "    \n",
    "    Detection hierarchy:\n",
    "    1. Honorifics (100% confidence)\n",
    "    2. Semantic keywords (95% confidence)\n",
    "    3. Common name list (80% confidence)\n",
    "    4. Unknown\n",
    "    \"\"\"\n",
    "    if pd.isna(username):\n",
    "        return 'unknown'\n",
    "    \n",
    "    username_str = str(username)\n",
    "    username_lower = username_str.lower()\n",
    "    gender = 'unknown'\n",
    "    \n",
    "    # TIER 1: Check honorifics (highest confidence)\n",
    "    for honorific in MALE_HONORIFICS:\n",
    "        if honorific in username_lower:\n",
    "            return 'male'\n",
    "    \n",
    "    for honorific in FEMALE_HONORIFICS:\n",
    "        if honorific in username_lower:\n",
    "            return 'female'\n",
    "    \n",
    "    # TIER 2: Check semantic keywords\n",
    "    for keyword in MALE_KEYWORDS:\n",
    "        if keyword in username_lower:\n",
    "            return 'male'\n",
    "    \n",
    "    for keyword in FEMALE_KEYWORDS:\n",
    "        if keyword in username_lower:\n",
    "            return 'female'\n",
    "    \n",
    "    # TIER 3: Split username and check against common names\n",
    "    parts = split_username_intelligent(username_str)\n",
    "    \n",
    "    for part in parts:\n",
    "        if part in COMMON_MALE_NAMES:\n",
    "            return 'male'\n",
    "        if part in COMMON_FEMALE_NAMES:\n",
    "            return 'female'\n",
    "    \n",
    "   \n",
    "\n",
    "    # TIER 4: Check if any name appears as substring in full username\n",
    "    username_clean = ''.join(parts) if parts else username_lower\n",
    "    for name in COMMON_MALE_NAMES:\n",
    "        if len(name) >= 4 and name in username_clean:  # Only check names 4+ chars\n",
    "            return 'male'\n",
    "    \n",
    "    for name in COMMON_FEMALE_NAMES:\n",
    "        if len(name) >= 4 and name in username_clean:\n",
    "            return 'female'\n",
    "\n",
    "    return 'unknown'\n",
    "\n",
    "# Test the improved function\n",
    "print(\"Testing improved gender detection:\")\n",
    "print(\"=\"*60)\n",
    "test_usernames = [\n",
    "    'JohnSmith1985',\n",
    "    'mary_reviews',\n",
    "    'Boba_Fett1138',\n",
    "    'kinglet',\n",
    "    'pastorjames',\n",
    "    'Her-Excellency',\n",
    "    'movieguy42',\n",
    "    'sarahloveshorror',\n",
    "    'randomuser999'\n",
    "]\n",
    "\n",
    "for username in test_usernames:\n",
    "    parts = split_username_intelligent(username)\n",
    "    gender = analyze_username_improved(username)\n",
    "    parts_str = str(parts)  # FIX: Convert list to string first\n",
    "    print(f\"{username:20} â†’ Parts: {parts_str:40} â†’ Gender: {gender}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying improved gender detection to all reviewers...\n",
      "(This will REPLACE the existing username_gender_hint column)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514c3d2f12084799b04669cc060f03c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GENDER DETECTION IMPROVEMENT\n",
      "============================================================\n",
      "\n",
      "Before (v1):\n",
      "  Identified: 767 (23.8%)\n",
      "username_gender_hint\n",
      "unknown    2455\n",
      "male        626\n",
      "female      141\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After (v2):\n",
      "  Identified: 767 (23.8%)\n",
      "username_gender_hint\n",
      "unknown    2455\n",
      "male        626\n",
      "female      141\n",
      "Name: count, dtype: int64\n",
      "\n",
      "âœ… Improvement: +0 reviewers identified (+0.0%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying improved gender detection to all reviewers...\")\n",
    "print(\"(This will REPLACE the existing username_gender_hint column)\\n\")\n",
    "\n",
    "# Store old values for comparison\n",
    "old_gender = df['username_gender_hint'].copy()\n",
    "\n",
    "# Apply improved detection\n",
    "df['username_gender_hint'] = df['Reviewer'].progress_apply(analyze_username_improved)\n",
    "\n",
    "# Stats comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENDER DETECTION IMPROVEMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "old_identified = (old_gender != 'unknown').sum()\n",
    "new_identified = (df['username_gender_hint'] != 'unknown').sum()\n",
    "\n",
    "print(f\"\\nBefore (v1):\")\n",
    "print(f\"  Identified: {old_identified} ({old_identified/len(df)*100:.1f}%)\")\n",
    "print(old_gender.value_counts())\n",
    "\n",
    "print(f\"\\nAfter (v2):\")\n",
    "print(f\"  Identified: {new_identified} ({new_identified/len(df)*100:.1f}%)\")\n",
    "print(df['username_gender_hint'].value_counts())\n",
    "\n",
    "improvement = new_identified - old_identified\n",
    "print(f\"\\nâœ… Improvement: +{improvement} reviewers identified (+{improvement/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Show some examples of newly identified reviewers\n",
    "newly_identified = df[(old_gender == 'unknown') & (df['username_gender_hint'] != 'unknown')]\n",
    "if len(newly_identified) > 0:\n",
    "    print(f\"\\nExample newly identified reviewers (first 10):\")\n",
    "    for idx, row in newly_identified.head(10).iterrows():\n",
    "        print(f\"  {row['Reviewer']:30} â†’ {row['username_gender_hint']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 2.6 enhanced gender detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODULE 2.5: Enhanced Gender Detection with Word Segmentation\n",
      "================================================================================\n",
      "\n",
      "Current gender detection coverage:\n",
      "  Identified: 767 / 3222 (23.8%)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Do you want to re-run gender detection with enhanced library? (yes/no):  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Installing required libraries...\n",
      "âœ… wordsegment library loaded\n",
      "ğŸ“¦ Loading word segmentation dictionary...\n",
      "âœ… Dictionary loaded\n",
      "âœ… names-dataset library loaded\n",
      "ğŸ“¦ Loading name database (this may take 10-15 seconds)...\n",
      "âœ… Name database loaded\n",
      "\n",
      "ğŸ§ª Testing enhanced detection on known examples:\n",
      "================================================================================\n",
      "âœ… isabelleanderson     â†’ female     | parts: ['Isabelle', 'Anderson']\n",
      "âœ… omarrangels          â†’ male       | parts: ['Omarr', 'Angels']\n",
      "âœ… Supercraig68         â†’ male       | parts: ['Super', 'Craig']\n",
      "âœ… purrlgurrl           â†’ female     | parts: ['Purr', 'Lgu', 'Rrl']\n",
      "âœ… karinafaolin         â†’ female     | parts: ['Karina', 'Aol']\n",
      "âœ… JohnSmith1985        â†’ male       | parts: ['John', 'Smith']\n",
      "âœ… sarahloveshorror     â†’ female     | parts: ['Sarah', 'Loves', 'Horror']\n",
      "âœ… christhecat          â†’ male       | parts: ['Chris', 'The', 'Cat']\n",
      "âœ… paulbenjamin         â†’ male       | parts: ['Paul', 'Benjamin']\n",
      "âœ… pastorjames          â†’ male       | parts: ['Pastor', 'James']\n",
      "âœ… Her-Excellency       â†’ female     | parts: ['Her', 'Excellency']\n",
      "================================================================================\n",
      "\n",
      "â³ Applying enhanced gender detection to all 3,222 reviewers...\n",
      "   This may take 3-5 minutes (word segmentation is slower)...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94212a07d403490192bf2d8f1ea0dbe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENDER DETECTION IMPROVEMENT\n",
      "================================================================================\n",
      "\n",
      "Before (original method):\n",
      "  Identified: 767 / 3222 (23.8%)\n",
      "username_gender_hint\n",
      "unknown    2455\n",
      "male        626\n",
      "female      141\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After (word segmentation + names-dataset):\n",
      "  Identified: 2590 / 3222 (80.4%)\n",
      "username_gender_hint\n",
      "male       2146\n",
      "unknown     632\n",
      "female      444\n",
      "Name: count, dtype: int64\n",
      "\n",
      "âœ… Improvement: +1823 reviewers identified (+56.6%)\n",
      "\n",
      "ğŸ“Š Example newly identified reviewers (first 30):\n",
      "  Boba_Fett1138                  â†’ male\n",
      "  todyun                         â†’ male\n",
      "  TakeTwoReviews                 â†’ male\n",
      "  LukeCustomer2                  â†’ male\n",
      "  kbnewton                       â†’ male\n",
      "  BA_Harrison                    â†’ male\n",
      "  glenmatisse                    â†’ male\n",
      "  moonspinner55                  â†’ female\n",
      "  HuntinPeck80                   â†’ male\n",
      "  gavin6942                      â†’ male\n",
      "  rzajac                         â†’ male\n",
      "  SnoopyStyle                    â†’ male\n",
      "  robb_772                       â†’ male\n",
      "  Bogmeister                     â†’ male\n",
      "  vicmackie                      â†’ male\n",
      "  Uriah43                        â†’ male\n",
      "  lambiepie-2                    â†’ male\n",
      "  maisannes                      â†’ female\n",
      "  shido-san                      â†’ male\n",
      "  badbrainsg                     â†’ male\n",
      "  Kirasjeri                      â†’ female\n",
      "  twcvcap1                       â†’ male\n",
      "  jboothmillard                  â†’ male\n",
      "  glenn-58                       â†’ male\n",
      "  tomousbourne                   â†’ male\n",
      "  sil-17                         â†’ female\n",
      "  budniewski1                    â†’ male\n",
      "  senortuffy                     â†’ male\n",
      "  xlars                          â†’ male\n",
      "  dvox                           â†’ male\n",
      "\n",
      "âœ… Enhanced gender detection complete!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODULE 2.5: ENHANCED GENDER DETECTION WITH WORD SEGMENTATION\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODULE 2.5: Enhanced Gender Detection with Word Segmentation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if we should re-run gender detection\n",
    "print(\"\\nCurrent gender detection coverage:\")\n",
    "current_coverage = (df['username_gender_hint'] != 'unknown').sum()\n",
    "print(f\"  Identified: {current_coverage} / {len(df)} ({current_coverage/len(df)*100:.1f}%)\")\n",
    "\n",
    "proceed = input(\"\\nDo you want to re-run gender detection with enhanced library? (yes/no): \")\n",
    "\n",
    "if proceed.lower() == 'yes':\n",
    "    print(\"\\nğŸ“Š Installing required libraries...\")\n",
    "    \n",
    "    # Install wordsegment for splitting compound words\n",
    "    import sys\n",
    "    try:\n",
    "        from wordsegment import load, segment\n",
    "        print(\"âœ… wordsegment library loaded\")\n",
    "    except ImportError:\n",
    "        print(\"âŒ wordsegment not found. Installing...\")\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"wordsegment\"])\n",
    "        from wordsegment import load, segment\n",
    "        print(\"âœ… wordsegment installed\")\n",
    "    \n",
    "    # Load wordsegment dictionary\n",
    "    print(\"ğŸ“¦ Loading word segmentation dictionary...\")\n",
    "    load()\n",
    "    print(\"âœ… Dictionary loaded\")\n",
    "    \n",
    "    # Install names-dataset\n",
    "    try:\n",
    "        from names_dataset import NameDataset\n",
    "        print(\"âœ… names-dataset library loaded\")\n",
    "    except ImportError:\n",
    "        print(\"âŒ names-dataset not found. Installing...\")\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"names-dataset\"])\n",
    "        from names_dataset import NameDataset\n",
    "        print(\"âœ… names-dataset installed\")\n",
    "    \n",
    "    # Initialize name detector\n",
    "    print(\"ğŸ“¦ Loading name database (this may take 10-15 seconds)...\")\n",
    "    nd = NameDataset()\n",
    "    print(\"âœ… Name database loaded\")\n",
    "    \n",
    "    # Keep existing detection functions for Tier 1-2\n",
    "    MALE_HONORIFICS = [\n",
    "        'mr', 'mister', 'sir', 'lord', 'king', 'prince', 'duke', 'baron',\n",
    "        'pastor', 'father', 'brother', 'monk', 'reverend', 'rabbi',\n",
    "        'captain', 'general', 'admiral', 'colonel'\n",
    "    ]\n",
    "    \n",
    "    FEMALE_HONORIFICS = [\n",
    "        'mrs', 'miss', 'ms', 'lady', 'queen', 'princess', 'duchess', 'baroness',\n",
    "        'sister', 'nun', 'mother', 'madam', 'dame',\n",
    "        'her-excellency', 'her-majesty', 'her-highness'\n",
    "    ]\n",
    "    \n",
    "    MALE_KEYWORDS = [\n",
    "        'guy', 'dude', 'bro', 'man', 'boy', 'lad', 'male', 'husband', 'dad', 'father'\n",
    "    ]\n",
    "    \n",
    "    FEMALE_KEYWORDS = [\n",
    "        'girl', 'grl', 'grrl', 'gurrl', 'gal', 'lady', 'woman', 'female', \n",
    "        'wife', 'mom', 'mother', 'chick', 'sis'\n",
    "    ]\n",
    "    \n",
    "    def split_username_intelligent(username):\n",
    "        \"\"\"\n",
    "        Split username using word segmentation to handle compound words.\n",
    "        This can split 'isabelleanderson' â†’ ['isabelle', 'anderson']\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Step 1: Replace separators with spaces\n",
    "        username = re.sub(r'[_\\-.]', ' ', username)\n",
    "        \n",
    "        # Step 2: Split on capital letters (CamelCase)\n",
    "        username = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', username)\n",
    "        \n",
    "        # Step 3: Split on numbers\n",
    "        username = re.sub(r'([a-zA-Z])([0-9])', r'\\1 \\2', username)\n",
    "        username = re.sub(r'([0-9])([a-zA-Z])', r'\\1 \\2', username)\n",
    "        \n",
    "        # Step 4: Split into initial parts\n",
    "        initial_parts = username.split()\n",
    "        \n",
    "        # Step 5: For each part, use word segmentation to split compound words\n",
    "        all_parts = []\n",
    "        for part in initial_parts:\n",
    "            # Skip numbers\n",
    "            if part.isdigit():\n",
    "                continue\n",
    "            \n",
    "            # If part has mixed case or is very short, keep it\n",
    "            if len(part) < 4:\n",
    "                all_parts.append(part)\n",
    "            elif part[0].isupper() and any(c.isupper() for c in part[1:]):\n",
    "                # Has CamelCase, already split above\n",
    "                all_parts.append(part)\n",
    "            else:\n",
    "                # Use word segmentation for compound words\n",
    "                # 'isabelleanderson' â†’ ['isabelle', 'anderson']\n",
    "                segmented = segment(part.lower())\n",
    "                all_parts.extend(segmented)\n",
    "        \n",
    "        # Step 6: Filter and capitalize\n",
    "        parts = [p.capitalize() for p in all_parts if len(p) >= 3]\n",
    "        \n",
    "        return parts\n",
    "    \n",
    "    def analyze_username_enhanced(username):\n",
    "        \"\"\"\n",
    "        Enhanced gender detection with word segmentation + names-dataset.\n",
    "        \n",
    "        Detection hierarchy:\n",
    "        1. Honorifics (100% confidence)\n",
    "        2. Semantic keywords (95% confidence)  \n",
    "        3. names-dataset on segmented name parts (80% confidence)\n",
    "        4. Unknown\n",
    "        \"\"\"\n",
    "        if pd.isna(username):\n",
    "            return 'unknown'\n",
    "        \n",
    "        username_str = str(username)\n",
    "        username_lower = username_str.lower()\n",
    "        \n",
    "        # TIER 1: Check honorifics (highest confidence)\n",
    "        for honorific in MALE_HONORIFICS:\n",
    "            if honorific in username_lower:\n",
    "                return 'male'\n",
    "        \n",
    "        for honorific in FEMALE_HONORIFICS:\n",
    "            if honorific in username_lower:\n",
    "                return 'female'\n",
    "        \n",
    "        # TIER 2: Check semantic keywords\n",
    "        for keyword in MALE_KEYWORDS:\n",
    "            if keyword in username_lower:\n",
    "                return 'male'\n",
    "        \n",
    "        for keyword in FEMALE_KEYWORDS:\n",
    "            if keyword in username_lower:\n",
    "                return 'female'\n",
    "        \n",
    "        # TIER 3: Use names-dataset on segmented parts\n",
    "        parts = split_username_intelligent(username_str)\n",
    "        \n",
    "        if parts:\n",
    "            # Check FIRST part first (most likely to be first name)\n",
    "            first_name_data = nd.search(parts[0])\n",
    "            \n",
    "            if first_name_data:\n",
    "                first_name_info = first_name_data.get('first_name')\n",
    "                \n",
    "                if first_name_info:\n",
    "                    gender_dict = first_name_info.get('gender')\n",
    "                    \n",
    "                    # gender_dict is like {'Female': 0.995, 'Male': 0.005}\n",
    "                    if gender_dict:\n",
    "                        male_prob = gender_dict.get('Male', 0)\n",
    "                        female_prob = gender_dict.get('Female', 0)\n",
    "                        \n",
    "                        # Need >60% confidence to assign gender\n",
    "                        if male_prob > 0.6:\n",
    "                            return 'male'\n",
    "                        elif female_prob > 0.6:\n",
    "                            return 'female'\n",
    "            \n",
    "            # Then check remaining parts (up to 3 parts to avoid false positives)\n",
    "            for part in parts[1:3]:\n",
    "                name_data = nd.search(part)\n",
    "                \n",
    "                if name_data:\n",
    "                    name_info = name_data.get('first_name')\n",
    "                    \n",
    "                    if name_info:\n",
    "                        gender_dict = name_info.get('gender')\n",
    "                        \n",
    "                        if gender_dict:\n",
    "                            male_prob = gender_dict.get('Male', 0)\n",
    "                            female_prob = gender_dict.get('Female', 0)\n",
    "                            \n",
    "                            if male_prob > 0.6:\n",
    "                                return 'male'\n",
    "                            elif female_prob > 0.6:\n",
    "                                return 'female'\n",
    "        \n",
    "        return 'unknown'\n",
    "    \n",
    "    # Test on known examples\n",
    "    print(\"\\nğŸ§ª Testing enhanced detection on known examples:\")\n",
    "    test_cases = [\n",
    "        ('isabelleanderson', 'female'),\n",
    "        ('omarrangels', 'male'),\n",
    "        ('Supercraig68', 'male'),\n",
    "        ('purrlgurrl', 'female'),\n",
    "        ('karinafaolin', 'female'),\n",
    "        ('JohnSmith1985', 'male'),\n",
    "        ('sarahloveshorror', 'female'),\n",
    "        ('christhecat', 'male'),\n",
    "        ('paulbenjamin', 'male'),\n",
    "        ('pastorjames', 'male'),\n",
    "        ('Her-Excellency', 'female')\n",
    "    ]\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    for username, expected in test_cases:\n",
    "        parts = split_username_intelligent(username)\n",
    "        result = analyze_username_enhanced(username)\n",
    "        status = \"âœ…\" if result == expected else \"âŒ\"\n",
    "        print(f\"{status} {username:20} â†’ {result:10} | parts: {parts}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Store old values for comparison\n",
    "    old_gender = df['username_gender_hint'].copy()\n",
    "    \n",
    "    # Apply enhanced detection to all reviewers\n",
    "    print(f\"\\nâ³ Applying enhanced gender detection to all {len(df):,} reviewers...\")\n",
    "    print(\"   This may take 3-5 minutes (word segmentation is slower)...\\n\")\n",
    "    \n",
    "    df['username_gender_hint'] = df['Reviewer'].progress_apply(analyze_username_enhanced)\n",
    "    \n",
    "    # Stats comparison\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENDER DETECTION IMPROVEMENT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    old_identified = (old_gender != 'unknown').sum()\n",
    "    new_identified = (df['username_gender_hint'] != 'unknown').sum()\n",
    "    \n",
    "    print(f\"\\nBefore (original method):\")\n",
    "    print(f\"  Identified: {old_identified} / {len(df)} ({old_identified/len(df)*100:.1f}%)\")\n",
    "    print(old_gender.value_counts())\n",
    "    \n",
    "    print(f\"\\nAfter (word segmentation + names-dataset):\")\n",
    "    print(f\"  Identified: {new_identified} / {len(df)} ({new_identified/len(df)*100:.1f}%)\")\n",
    "    print(df['username_gender_hint'].value_counts())\n",
    "    \n",
    "    improvement = new_identified - old_identified\n",
    "    print(f\"\\nâœ… Improvement: +{improvement} reviewers identified (+{improvement/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Show some examples of newly identified reviewers\n",
    "    newly_identified = df[(old_gender == 'unknown') & (df['username_gender_hint'] != 'unknown')]\n",
    "    if len(newly_identified) > 0:\n",
    "        print(f\"\\nğŸ“Š Example newly identified reviewers (first 30):\")\n",
    "        for idx, row in newly_identified.head(30).iterrows():\n",
    "            print(f\"  {row['Reviewer']:30} â†’ {row['username_gender_hint']}\")\n",
    "    \n",
    "    print(\"\\nâœ… Enhanced gender detection complete!\")\n",
    "    \n",
    "else:\n",
    "    print(\"â­ï¸  Skipping enhanced gender detection, keeping existing results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Module 5: Emotion Detection (NEW)\n",
    "\n",
    "**Goal**: Add 8 emotion columns using NRCLex\n",
    "\n",
    "**Method**: NRCLex (NRC Emotion Lexicon)\n",
    "- Based on 14,000+ words with emotion associations\n",
    "- Provides 8 core emotions: joy, trust, fear, surprise, sadness, disgust, anger, anticipation\n",
    "- Complements VADER with specific emotion breakdowns\n",
    "\n",
    "**New Columns**: 8 total\n",
    "- `emotion_joy` - Joy/happiness score (0 to 1)\n",
    "- `emotion_trust` - Trust/acceptance score (0 to 1)\n",
    "- `emotion_fear` - Fear/anxiety score (0 to 1)\n",
    "- `emotion_surprise` - Surprise/amazement score (0 to 1)\n",
    "- `emotion_sadness` - Sadness/sorrow score (0 to 1)\n",
    "- `emotion_disgust` - Disgust/loathing score (0 to 1)\n",
    "- `emotion_anger` - Anger/rage score (0 to 1)\n",
    "- `emotion_anticipation` - Anticipation/interest score (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Emotion columns already exist, skipping...\n",
      "   Existing columns: ['emotion_joy', 'emotion_trust', 'emotion_fear', 'emotion_surprise', 'emotion_sadness', 'emotion_disgust', 'emotion_anger', 'emotion_anticipation']\n"
     ]
    }
   ],
   "source": [
    "# Check if emotion columns already exist\n",
    "emotion_cols = ['emotion_joy', 'emotion_trust', 'emotion_fear', 'emotion_surprise',\n",
    "                'emotion_sadness', 'emotion_disgust', 'emotion_anger', 'emotion_anticipation']\n",
    "\n",
    "if all(col in df.columns for col in emotion_cols):\n",
    "    print(\"âœ… Emotion columns already exist, skipping...\")\n",
    "    print(f\"   Existing columns: {emotion_cols}\")\n",
    "    SKIP_EMOTIONS = True\n",
    "else:\n",
    "    print(\"Adding emotion detection (NEW FEATURE)...\")\n",
    "    SKIP_EMOTIONS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_EMOTIONS:\n",
    "    def extract_emotions(text):\n",
    "        \"\"\"\n",
    "        Extract emotion scores using NRCLex.\n",
    "        Returns dict with 8 emotion scores.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Initialize NRCLex with review text\n",
    "            emotion_obj = NRCLex(str(text))\n",
    "            \n",
    "            # Get affect frequencies (normalized 0-1)\n",
    "            emotions = emotion_obj.affect_frequencies\n",
    "            \n",
    "            return {\n",
    "                'emotion_joy': emotions.get('joy', 0.0),\n",
    "                'emotion_trust': emotions.get('trust', 0.0),\n",
    "                'emotion_fear': emotions.get('fear', 0.0),\n",
    "                'emotion_surprise': emotions.get('surprise', 0.0),\n",
    "                'emotion_sadness': emotions.get('sadness', 0.0),\n",
    "                'emotion_disgust': emotions.get('disgust', 0.0),\n",
    "                'emotion_anger': emotions.get('anger', 0.0),\n",
    "                'emotion_anticipation': emotions.get('anticipation', 0.0)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            # Return zeros on error\n",
    "            return {\n",
    "                'emotion_joy': 0.0,\n",
    "                'emotion_trust': 0.0,\n",
    "                'emotion_fear': 0.0,\n",
    "                'emotion_surprise': 0.0,\n",
    "                'emotion_sadness': 0.0,\n",
    "                'emotion_disgust': 0.0,\n",
    "                'emotion_anger': 0.0,\n",
    "                'emotion_anticipation': 0.0\n",
    "            }\n",
    "    \n",
    "    # Test on a sample review first\n",
    "    print(\"Testing emotion detection on sample review:\")\n",
    "    sample_text = df.iloc[0]['Review_Text']\n",
    "    sample_emotions = extract_emotions(sample_text)\n",
    "    print(f\"\\nSample emotions:\")\n",
    "    for emotion, score in sample_emotions.items():\n",
    "        print(f\"  {emotion:20}: {score:.3f}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Test successful! Now processing all {len(df):,} reviews...\")\n",
    "    print(\"â±ï¸  This may take 2-3 minutes...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Emotion columns already exist, skipped processing\n"
     ]
    }
   ],
   "source": [
    "if not SKIP_EMOTIONS:\n",
    "    print(\"Extracting emotions from all reviews...\\n\")\n",
    "    \n",
    "    # Apply emotion extraction to all reviews\n",
    "    emotion_results = df['Review_Text'].progress_apply(extract_emotions)\n",
    "    emotion_df = pd.DataFrame(emotion_results.tolist())\n",
    "    \n",
    "    # Add emotion columns to main dataframe\n",
    "    df = pd.concat([df, emotion_df], axis=1)\n",
    "    \n",
    "    # Stats\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EMOTION DETECTION COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nSuccess rate: {(emotion_df['emotion_joy'].notna().sum() / len(df) * 100):.1f}%\")\n",
    "    \n",
    "    print(f\"\\nAverage emotion scores across all reviews:\")\n",
    "    for col in emotion_cols:\n",
    "        avg = df[col].mean()\n",
    "        print(f\"  {col:25}: {avg:.3f}\")\n",
    "    \n",
    "    # Find review with highest joy\n",
    "    max_joy_idx = df['emotion_joy'].idxmax()\n",
    "    max_joy_review = df.loc[max_joy_idx]\n",
    "    print(f\"\\nğŸ“Š Review with highest JOY score ({max_joy_review['emotion_joy']:.3f}):\")\n",
    "    print(f\"   Movie: {max_joy_review['Movie_Title']}\")\n",
    "    print(f\"   Rating: {max_joy_review['Rating']}/10\")\n",
    "    print(f\"   Reviewer: {max_joy_review['Reviewer']}\")\n",
    "    \n",
    "    # Find review with highest fear\n",
    "    max_fear_idx = df['emotion_fear'].idxmax()\n",
    "    max_fear_review = df.loc[max_fear_idx]\n",
    "    print(f\"\\nğŸ˜¨ Review with highest FEAR score ({max_fear_review['emotion_fear']:.3f}):\")\n",
    "    print(f\"   Movie: {max_fear_review['Movie_Title']}\")\n",
    "    print(f\"   Rating: {max_fear_review['Rating']}/10\")\n",
    "    print(f\"   Reviewer: {max_fear_review['Reviewer']}\")\n",
    "else:\n",
    "    print(\"âœ… Emotion columns already exist, skipped processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Modual 6 Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODULE 6: Writing Complexity & Readability Analysis\n",
      "================================================================================\n",
      "âœ… Writing complexity features already exist, skipping Module 6...\n",
      "   Found columns: flesch_reading_ease, flesch_kincaid_grade, etc.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODULE 6: WRITING COMPLEXITY & READABILITY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODULE 6: Writing Complexity & Readability Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'flesch_reading_ease' not in df.columns:\n",
    "    print(\"\\nğŸ“Š Extracting writing complexity features...\")\n",
    "    \n",
    "    # Import required libraries\n",
    "    try:\n",
    "        import textstat\n",
    "        print(\"âœ… textstat library loaded\")\n",
    "    except ImportError:\n",
    "        print(\"âŒ textstat not found. Installing...\")\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"textstat\"])\n",
    "        import textstat\n",
    "        print(\"âœ… textstat installed and loaded\")\n",
    "    \n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    import re\n",
    "    \n",
    "    # Initialize new columns\n",
    "    df['flesch_reading_ease'] = None\n",
    "    df['flesch_kincaid_grade'] = None\n",
    "    df['avg_sentence_length'] = None\n",
    "    df['avg_word_length'] = None\n",
    "    df['type_token_ratio'] = None\n",
    "    df['long_word_percentage'] = None\n",
    "    df['complex_word_count'] = None\n",
    "    df['syllable_count'] = None\n",
    "    \n",
    "    # Process each review\n",
    "    for idx, row in df.iterrows():\n",
    "        text = str(row['Review_Text'])\n",
    "        \n",
    "        if len(text.strip()) == 0:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Readability metrics (textstat handles edge cases)\n",
    "            df.at[idx, 'flesch_reading_ease'] = textstat.flesch_reading_ease(text)\n",
    "            df.at[idx, 'flesch_kincaid_grade'] = textstat.flesch_kincaid_grade(text)\n",
    "            \n",
    "            # Sentence-level metrics\n",
    "            sentences = sent_tokenize(text)\n",
    "            words = word_tokenize(text.lower())\n",
    "            words_clean = [w for w in words if w.isalnum()]  # Remove punctuation\n",
    "            \n",
    "            if len(sentences) > 0 and len(words_clean) > 0:\n",
    "                # Average sentence length\n",
    "                df.at[idx, 'avg_sentence_length'] = len(words_clean) / len(sentences)\n",
    "                \n",
    "                # Average word length\n",
    "                df.at[idx, 'avg_word_length'] = sum(len(w) for w in words_clean) / len(words_clean)\n",
    "                \n",
    "                # Lexical diversity (Type-Token Ratio)\n",
    "                unique_words = set(words_clean)\n",
    "                df.at[idx, 'type_token_ratio'] = len(unique_words) / len(words_clean)\n",
    "                \n",
    "                # Long words (7+ characters)\n",
    "                long_words = [w for w in words_clean if len(w) >= 7]\n",
    "                df.at[idx, 'long_word_percentage'] = (len(long_words) / len(words_clean)) * 100\n",
    "                \n",
    "                # Complex words (3+ syllables) - using textstat\n",
    "                df.at[idx, 'complex_word_count'] = textstat.difficult_words(text)\n",
    "                \n",
    "                # Total syllables\n",
    "                df.at[idx, 'syllable_count'] = textstat.syllable_count(text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Silent failure for individual reviews\n",
    "            continue\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (idx + 1) % 500 == 0:\n",
    "            print(f\"   Processed {idx + 1:,} / {len(df):,} reviews...\")\n",
    "    \n",
    "    print(f\"âœ… Completed processing {len(df):,} reviews\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nğŸ“ˆ Writing Complexity Summary:\")\n",
    "    print(f\"   Mean Flesch Reading Ease: {df['flesch_reading_ease'].mean():.1f} (0-100, higher=easier)\")\n",
    "    print(f\"   Mean Grade Level: {df['flesch_kincaid_grade'].mean():.1f} (U.S. grade)\")\n",
    "    print(f\"   Mean Sentence Length: {df['avg_sentence_length'].mean():.1f} words\")\n",
    "    print(f\"   Mean Word Length: {df['avg_word_length'].mean():.1f} characters\")\n",
    "    print(f\"   Mean Type-Token Ratio: {df['type_token_ratio'].mean():.3f} (vocabulary richness)\")\n",
    "    print(f\"   Mean Long Word %: {df['long_word_percentage'].mean():.1f}%\")\n",
    "    \n",
    "    # Interpretation guide\n",
    "    print(\"\\nğŸ“š Flesch Reading Ease Interpretation:\")\n",
    "    print(\"   90-100: Very Easy (5th grade)\")\n",
    "    print(\"   80-89:  Easy (6th grade)\")\n",
    "    print(\"   70-79:  Fairly Easy (7th grade)\")\n",
    "    print(\"   60-69:  Standard (8th-9th grade)\")\n",
    "    print(\"   50-59:  Fairly Difficult (10th-12th grade)\")\n",
    "    print(\"   30-49:  Difficult (College)\")\n",
    "    print(\"   0-29:   Very Confusing (College graduate)\")\n",
    "    \n",
    "else:\n",
    "    print(\"âœ… Writing complexity features already exist, skipping Module 6...\")\n",
    "    print(f\"   Found columns: flesch_reading_ease, flesch_kincaid_grade, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 7 TEMPORAL & ENGAGEMENT FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODULE 7: Temporal & Engagement Analysis\n",
      "================================================================================\n",
      "âœ… Temporal & engagement features already exist, skipping Module 7...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODULE 7: TEMPORAL & ENGAGEMENT FEATURES\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODULE 7: Temporal & Engagement Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'years_since_release' not in df.columns:\n",
    "    print(\"\\nâ° Calculating temporal and engagement features...\")\n",
    "    \n",
    "    # Movie release year mapping (from your data)\n",
    "    MOVIE_RELEASE_YEARS = {\n",
    "        'Angel Heart': 1987,\n",
    "        'The Rapture': 1991,\n",
    "        'Lady in the Water': 2006,\n",
    "        'Antichrist': 2009,\n",
    "        'The Witch': 2015,\n",
    "        'We Are Still Here': 2015,\n",
    "        'The Wailing': 2016,\n",
    "        'A Dark Song': 2016,\n",
    "        'The Endless': 2017,\n",
    "        'Tigers Are Not Afraid': 2017,\n",
    "        'The Ritual': 2017,\n",
    "        'Hagazussa': 2017,\n",
    "        'Annihilation': 2018,\n",
    "        'Apostle': 2018,\n",
    "        'Hereditary': 2018,\n",
    "        'The Wind': 2018,\n",
    "        'Midsommar': 2019,\n",
    "        'His House': 2020,\n",
    "        'The Medium': 2021,\n",
    "        'The Watchers': 2024\n",
    "    }\n",
    "    \n",
    "    # Initialize columns\n",
    "    df['movie_release_year'] = None\n",
    "    df['review_year'] = None\n",
    "    df['years_since_release'] = None\n",
    "    df['review_window'] = None\n",
    "    df['total_votes'] = None\n",
    "    df['helpfulness_ratio'] = None\n",
    "    df['vote_polarization'] = None\n",
    "    df['has_engagement'] = None\n",
    "    \n",
    "    # Process each review\n",
    "    for idx, row in df.iterrows():\n",
    "        movie = row['Movie_Title']\n",
    "        review_date = pd.to_datetime(row['Review_Date'])\n",
    "        \n",
    "        # Movie release year\n",
    "        release_year = MOVIE_RELEASE_YEARS.get(movie)\n",
    "        if release_year:\n",
    "            df.at[idx, 'movie_release_year'] = release_year\n",
    "            df.at[idx, 'review_year'] = review_date.year\n",
    "            \n",
    "            # Years between release and review\n",
    "            years_diff = review_date.year - release_year\n",
    "            df.at[idx, 'years_since_release'] = years_diff\n",
    "            \n",
    "            # Categorize review timing window\n",
    "            if years_diff <= 0:\n",
    "                df.at[idx, 'review_window'] = 'Opening Year'\n",
    "            elif years_diff == 1:\n",
    "                df.at[idx, 'review_window'] = 'Year 2'\n",
    "            elif years_diff <= 3:\n",
    "                df.at[idx, 'review_window'] = 'Years 2-3'\n",
    "            elif years_diff <= 5:\n",
    "                df.at[idx, 'review_window'] = 'Years 4-5'\n",
    "            else:\n",
    "                df.at[idx, 'review_window'] = '5+ Years'\n",
    "        \n",
    "        # Engagement metrics\n",
    "        up_votes = row['Helpful_Votes_Up'] if pd.notna(row['Helpful_Votes_Up']) else 0\n",
    "        down_votes = row['Helpful_Votes_Down'] if pd.notna(row['Helpful_Votes_Down']) else 0\n",
    "        \n",
    "        total = up_votes + down_votes\n",
    "        df.at[idx, 'total_votes'] = total\n",
    "        df.at[idx, 'has_engagement'] = total > 0\n",
    "        \n",
    "        if total > 0:\n",
    "            df.at[idx, 'helpfulness_ratio'] = up_votes / total\n",
    "            df.at[idx, 'vote_polarization'] = abs(up_votes - down_votes)\n",
    "        else:\n",
    "            df.at[idx, 'helpfulness_ratio'] = None\n",
    "            df.at[idx, 'vote_polarization'] = 0\n",
    "    \n",
    "    print(f\"âœ… Processed {len(df):,} reviews\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nğŸ“ˆ Temporal Analysis Summary:\")\n",
    "    print(f\"   Mean years since release: {df['years_since_release'].mean():.1f} years\")\n",
    "    print(f\"   Median years since release: {df['years_since_release'].median():.1f} years\")\n",
    "    print(f\"   Reviews written in opening year: {(df['years_since_release'] <= 0).sum():,} ({(df['years_since_release'] <= 0).sum()/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nğŸ“Š Review Window Distribution:\")\n",
    "    print(df['review_window'].value_counts().sort_index())\n",
    "    \n",
    "    print(\"\\nğŸ‘ Engagement Summary:\")\n",
    "    print(f\"   Reviews with votes: {df['has_engagement'].sum():,} ({df['has_engagement'].sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Mean helpfulness ratio: {df['helpfulness_ratio'].mean():.3f} (among voted reviews)\")\n",
    "    print(f\"   Mean total votes: {df['total_votes'].mean():.1f}\")\n",
    "    print(f\"   Median vote polarization: {df['vote_polarization'].median():.0f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âœ… Temporal & engagement features already exist, skipping Module 7...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 8 Review Structure and Punctuation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODULE 8: Review Structure & Punctuation Analysis\n",
      "================================================================================\n",
      "âœ… Structure & punctuation features already exist, skipping Module 8...\n",
      "   Found columns: paragraph_count, exclamation_count, caps_word_count, etc.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODULE 8: REVIEW STRUCTURE & PUNCTUATION ANALYSIS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODULE 8: Review Structure & Punctuation Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'paragraph_count' not in df.columns:\n",
    "    print(\"\\nğŸ“ Extracting structural and punctuation features...\")\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    # Initialize new columns\n",
    "    df['paragraph_count'] = None\n",
    "    df['avg_paragraph_length'] = None\n",
    "    df['exclamation_count'] = None\n",
    "    df['question_mark_count'] = None\n",
    "    df['ellipsis_count'] = None\n",
    "    df['caps_word_count'] = None\n",
    "    df['quote_count'] = None\n",
    "    df['double_quote_count'] = None\n",
    "    df['single_quote_count'] = None\n",
    "    df['punctuation_density'] = None\n",
    "    df['uppercase_ratio'] = None\n",
    "    \n",
    "    # Process each review\n",
    "    for idx, row in df.iterrows():\n",
    "        text = str(row['Review_Text'])\n",
    "        review_length = row['Review_Length']\n",
    "        \n",
    "        if len(text.strip()) == 0 or review_length == 0:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Paragraph structure (split on double newlines or multiple newlines)\n",
    "            paragraphs = re.split(r'\\n\\s*\\n', text.strip())\n",
    "            paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
    "            para_count = len(paragraphs) if paragraphs else 1\n",
    "            \n",
    "            df.at[idx, 'paragraph_count'] = para_count\n",
    "            df.at[idx, 'avg_paragraph_length'] = review_length / para_count\n",
    "            \n",
    "            # Exclamation marks (emotional intensity)\n",
    "            df.at[idx, 'exclamation_count'] = text.count('!')\n",
    "            \n",
    "            # Question marks (engagement, rhetorical questions)\n",
    "            df.at[idx, 'question_mark_count'] = text.count('?')\n",
    "            \n",
    "            # Ellipsis (dramatic pauses, trailing thoughts)\n",
    "            ellipsis_pattern = r'\\.{3,}|â€¦'\n",
    "            df.at[idx, 'ellipsis_count'] = len(re.findall(ellipsis_pattern, text))\n",
    "            \n",
    "            # ALL CAPS words (emphasis, shouting)\n",
    "            caps_pattern = r'\\b[A-Z]{2,}\\b'\n",
    "            caps_words = re.findall(caps_pattern, text)\n",
    "            # Filter out common acronyms\n",
    "            common_acronyms = {'DVD', 'VHS', 'TV', 'CGI', 'IMDb', 'USA', 'UK', 'US'}\n",
    "            caps_words = [w for w in caps_words if w not in common_acronyms]\n",
    "            df.at[idx, 'caps_word_count'] = len(caps_words)\n",
    "            \n",
    "            # Quote usage (dialogue/scene references)\n",
    "            df.at[idx, 'double_quote_count'] = text.count('\"')\n",
    "            df.at[idx, 'single_quote_count'] = text.count(\"'\")\n",
    "            df.at[idx, 'quote_count'] = text.count('\"') + text.count(\"'\")\n",
    "            \n",
    "            # Overall punctuation density (per 100 words)\n",
    "            punctuation_chars = '!?.,:;-â€”'\n",
    "            punct_count = sum(text.count(p) for p in punctuation_chars)\n",
    "            words_estimate = review_length  # Review_Length is word count\n",
    "            df.at[idx, 'punctuation_density'] = (punct_count / words_estimate * 100) if words_estimate > 0 else 0\n",
    "            \n",
    "            # Uppercase letter ratio (intensity metric)\n",
    "            letters = [c for c in text if c.isalpha()]\n",
    "            if letters:\n",
    "                uppercase_count = sum(1 for c in letters if c.isupper())\n",
    "                df.at[idx, 'uppercase_ratio'] = uppercase_count / len(letters)\n",
    "            else:\n",
    "                df.at[idx, 'uppercase_ratio'] = 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Silent failure for individual reviews\n",
    "            continue\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (idx + 1) % 500 == 0:\n",
    "            print(f\"   Processed {idx + 1:,} / {len(df):,} reviews...\")\n",
    "    \n",
    "    print(f\"âœ… Completed processing {len(df):,} reviews\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nğŸ“ˆ Structure & Punctuation Summary:\")\n",
    "    print(f\"   Mean paragraph count: {df['paragraph_count'].mean():.1f}\")\n",
    "    print(f\"   Mean paragraph length: {df['avg_paragraph_length'].mean():.1f} words\")\n",
    "    print(f\"   Reviews with exclamations: {(df['exclamation_count'] > 0).sum():,} ({(df['exclamation_count'] > 0).sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Reviews with questions: {(df['question_mark_count'] > 0).sum():,} ({(df['question_mark_count'] > 0).sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Reviews with ellipsis: {(df['ellipsis_count'] > 0).sum():,} ({(df['ellipsis_count'] > 0).sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Reviews with CAPS words: {(df['caps_word_count'] > 0).sum():,} ({(df['caps_word_count'] > 0).sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Mean punctuation density: {df['punctuation_density'].mean():.1f} marks per 100 words\")\n",
    "    \n",
    "    print(\"\\nğŸ”¥ Intensity Indicators:\")\n",
    "    print(f\"   Mean exclamations per review: {df['exclamation_count'].mean():.1f}\")\n",
    "    print(f\"   Max exclamations in one review: {df['exclamation_count'].max():.0f}\")\n",
    "    print(f\"   Mean CAPS words per review: {df['caps_word_count'].mean():.1f}\")\n",
    "    print(f\"   Reviews with 5+ exclamations: {(df['exclamation_count'] >= 5).sum():,} (very emotional)\")\n",
    "    \n",
    "    print(\"\\nğŸ“Š Quote Usage (dialogue/scene references):\")\n",
    "    print(f\"   Reviews with quotes: {(df['quote_count'] > 0).sum():,} ({(df['quote_count'] > 0).sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Mean quotes per review: {df['quote_count'].mean():.1f}\")\n",
    "    \n",
    "    # Find most punctuation-heavy review\n",
    "    max_punct_idx = df['punctuation_density'].idxmax()\n",
    "    max_punct_review = df.loc[max_punct_idx]\n",
    "    print(f\"\\nâš¡ Most punctuation-heavy review ({max_punct_review['punctuation_density']:.1f} marks/100 words):\")\n",
    "    print(f\"   Movie: {max_punct_review['Movie_Title']}\")\n",
    "    print(f\"   Rating: {max_punct_review['Rating']}/10\")\n",
    "    print(f\"   Exclamations: {max_punct_review['exclamation_count']:.0f}\")\n",
    "    print(f\"   Questions: {max_punct_review['question_mark_count']:.0f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âœ… Structure & punctuation features already exist, skipping Module 8...\")\n",
    "    print(f\"   Found columns: paragraph_count, exclamation_count, caps_word_count, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Module 9 Part-of-speech analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODULE 9: Part-of-Speech Analysis\n",
      "================================================================================\n",
      "âœ… POS analysis features already exist, skipping Module 9...\n",
      "   Found columns: adj_ratio, verb_ratio, noun_ratio, etc.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODULE 9: PART-OF-SPEECH (POS) ANALYSIS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODULE 9: Part-of-Speech Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'adj_ratio' not in df.columns:\n",
    "    print(\"\\nğŸ”¤ Extracting POS ratios (writing style analysis)...\")\n",
    "    print(\"â±ï¸  This uses spaCy and may take 3-5 minutes...\\n\")\n",
    "    \n",
    "    # Import spaCy\n",
    "    try:\n",
    "        import spacy\n",
    "        print(\"âœ… spaCy library loaded\")\n",
    "    except ImportError:\n",
    "        print(\"âŒ spaCy not found. Installing...\")\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"spacy\"])\n",
    "        import spacy\n",
    "        print(\"âœ… spaCy installed\")\n",
    "    \n",
    "    # Load English model\n",
    "    try:\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        print(\"âœ… English model loaded\\n\")\n",
    "    except OSError:\n",
    "        print(\"ğŸ“¦ Downloading spaCy English model...\")\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        print(\"âœ… English model loaded\\n\")\n",
    "    \n",
    "    # Disable unnecessary pipeline components for speed\n",
    "    nlp.disable_pipes(['parser', 'ner'])\n",
    "    \n",
    "    # Initialize new columns\n",
    "    df['adj_ratio'] = None          # Adjectives (descriptive)\n",
    "    df['verb_ratio'] = None         # Verbs (action-oriented)\n",
    "    df['noun_ratio'] = None         # Nouns (analytical/factual)\n",
    "    df['adverb_ratio'] = None       # Adverbs (modifier-heavy)\n",
    "    df['pronoun_ratio'] = None      # Pronouns (personal vs objective)\n",
    "    df['first_person_ratio'] = None # I/me/my/we/us/our (subjective)\n",
    "    df['second_person_ratio'] = None # You/your (direct address)\n",
    "    df['determiner_ratio'] = None   # The/a/an (specificity)\n",
    "    df['conjunction_ratio'] = None  # And/but/or (complexity)\n",
    "    \n",
    "    # Process each review\n",
    "    for idx, row in df.iterrows():\n",
    "        text = str(row['Review_Text'])\n",
    "        \n",
    "        if len(text.strip()) == 0:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Process with spaCy (limit to first 1M characters to avoid crashes)\n",
    "            doc = nlp(text[:1000000])\n",
    "            \n",
    "            # Count POS tags\n",
    "            total_tokens = len([token for token in doc if not token.is_punct and not token.is_space])\n",
    "            \n",
    "            if total_tokens == 0:\n",
    "                continue\n",
    "            \n",
    "            # POS tag counts\n",
    "            adj_count = sum(1 for token in doc if token.pos_ == 'ADJ')\n",
    "            verb_count = sum(1 for token in doc if token.pos_ == 'VERB')\n",
    "            noun_count = sum(1 for token in doc if token.pos_ in ['NOUN', 'PROPN'])\n",
    "            adverb_count = sum(1 for token in doc if token.pos_ == 'ADV')\n",
    "            pronoun_count = sum(1 for token in doc if token.pos_ == 'PRON')\n",
    "            det_count = sum(1 for token in doc if token.pos_ == 'DET')\n",
    "            conj_count = sum(1 for token in doc if token.pos_ in ['CCONJ', 'SCONJ'])\n",
    "            \n",
    "            # Calculate ratios (per 100 words for interpretability)\n",
    "            df.at[idx, 'adj_ratio'] = (adj_count / total_tokens) * 100\n",
    "            df.at[idx, 'verb_ratio'] = (verb_count / total_tokens) * 100\n",
    "            df.at[idx, 'noun_ratio'] = (noun_count / total_tokens) * 100\n",
    "            df.at[idx, 'adverb_ratio'] = (adverb_count / total_tokens) * 100\n",
    "            df.at[idx, 'pronoun_ratio'] = (pronoun_count / total_tokens) * 100\n",
    "            df.at[idx, 'determiner_ratio'] = (det_count / total_tokens) * 100\n",
    "            df.at[idx, 'conjunction_ratio'] = (conj_count / total_tokens) * 100\n",
    "            \n",
    "            # Specific pronoun analysis\n",
    "            first_person = ['i', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves']\n",
    "            second_person = ['you', 'your', 'yours', 'yourself', 'yourselves']\n",
    "            \n",
    "            first_person_count = sum(1 for token in doc if token.lower_ in first_person)\n",
    "            second_person_count = sum(1 for token in doc if token.lower_ in second_person)\n",
    "            \n",
    "            df.at[idx, 'first_person_ratio'] = (first_person_count / total_tokens) * 100\n",
    "            df.at[idx, 'second_person_ratio'] = (second_person_count / total_tokens) * 100\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Silent failure for individual reviews\n",
    "            continue\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (idx + 1) % 500 == 0:\n",
    "            print(f\"   Processed {idx + 1:,} / {len(df):,} reviews...\")\n",
    "    \n",
    "    print(f\"âœ… Completed processing {len(df):,} reviews\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nğŸ“ˆ POS Analysis Summary:\")\n",
    "    print(f\"   Mean adjective ratio: {df['adj_ratio'].mean():.1f}% (descriptive writing)\")\n",
    "    print(f\"   Mean verb ratio: {df['verb_ratio'].mean():.1f}% (action-oriented)\")\n",
    "    print(f\"   Mean noun ratio: {df['noun_ratio'].mean():.1f}% (factual/analytical)\")\n",
    "    print(f\"   Mean adverb ratio: {df['adverb_ratio'].mean():.1f}% (modifier-heavy)\")\n",
    "    print(f\"   Mean pronoun ratio: {df['pronoun_ratio'].mean():.1f}%\")\n",
    "    \n",
    "    print(\"\\nğŸ‘¤ Voice Analysis:\")\n",
    "    print(f\"   Mean 1st person ratio: {df['first_person_ratio'].mean():.1f}% (subjective/personal)\")\n",
    "    print(f\"   Mean 2nd person ratio: {df['second_person_ratio'].mean():.1f}% (direct address)\")\n",
    "    \n",
    "    print(\"\\nğŸ“Š Writing Style Indicators:\")\n",
    "    print(f\"   High 1st person (>5%): {(df['first_person_ratio'] > 5).sum():,} reviews (very personal)\")\n",
    "    print(f\"   High adjectives (>10%): {(df['adj_ratio'] > 10).sum():,} reviews (descriptive)\")\n",
    "    print(f\"   High nouns (>25%): {(df['noun_ratio'] > 25).sum():,} reviews (analytical)\")\n",
    "    print(f\"   Low 1st person (<2%): {(df['first_person_ratio'] < 2).sum():,} reviews (objective/critical)\")\n",
    "    \n",
    "    # Interpretation guide\n",
    "    print(\"\\nğŸ“š Writing Style Interpretation:\")\n",
    "    print(\"   High adjectives + high 1st person = Emotional/subjective reviews\")\n",
    "    print(\"   High nouns + low 1st person = Analytical/critical reviews\")\n",
    "    print(\"   High verbs = Action/plot-focused reviews\")\n",
    "    print(\"   High adverbs = Nuanced/qualified opinions\")\n",
    "    print(\"   High 2nd person = Direct engagement with reader\")\n",
    "    \n",
    "    # Find most subjective review\n",
    "    max_subjective_idx = df['first_person_ratio'].idxmax()\n",
    "    max_subjective = df.loc[max_subjective_idx]\n",
    "    print(f\"\\nğŸ‘¤ Most subjective review ({max_subjective['first_person_ratio']:.1f}% first-person):\")\n",
    "    print(f\"   Movie: {max_subjective['Movie_Title']}\")\n",
    "    print(f\"   Rating: {max_subjective['Rating']}/10\")\n",
    "    print(f\"   Reviewer: {max_subjective['Reviewer']}\")\n",
    "    \n",
    "    # Find most objective review\n",
    "    min_subjective_idx = df['first_person_ratio'].idxmin()\n",
    "    min_subjective = df.loc[min_subjective_idx]\n",
    "    print(f\"\\nğŸ“Š Most objective review ({min_subjective['first_person_ratio']:.1f}% first-person):\")\n",
    "    print(f\"   Movie: {min_subjective['Movie_Title']}\")\n",
    "    print(f\"   Rating: {min_subjective['Rating']}/10\")\n",
    "    print(f\"   Reviewer: {min_subjective['Reviewer']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âœ… POS analysis features already exist, skipping Module 9...\")\n",
    "    print(f\"   Found columns: adj_ratio, verb_ratio, noun_ratio, etc.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Final Summary & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INCREMENTAL FEATURE EXTRACTION SUMMARY\n",
      "============================================================\n",
      "\n",
      "Total reviews: 3,222\n",
      "Total columns: 76\n",
      "\n",
      "ğŸ¯ Features Improved/Added:\n",
      "  âœ… Gender Detection (v2): 2590 identified (80.4%)\n",
      "  â­ï¸  Emotion Detection: Already existed, skipped\n",
      "\n",
      "ğŸ“‹ Complete Feature List (76 columns):\n",
      "['Review_ID', 'Movie_Title', 'Source', 'Reviewer', 'Review_Date', 'Rating', 'Review_Title', 'Review_Text', 'Review_Length', 'Helpful_Votes_Up', 'Helpful_Votes_Down', 'Spoiler_Flag', 'vader_compound', 'vader_pos', 'vader_neg', 'vader_neu', 'username_gender_hint', 'username_age_hint', 'username_interests', 'username_patterns', 'movies_mentioned', 'movie_mention_count', 'has_comparisons', 'comparison_context', 'love_statements', 'love_count', 'hate_statements', 'hate_count', 'wish_statements', 'wish_count', 'questions', 'question_count', 'emotion_joy', 'emotion_trust', 'emotion_fear', 'emotion_surprise', 'emotion_sadness', 'emotion_disgust', 'emotion_anger', 'emotion_anticipation', 'flesch_reading_ease', 'flesch_kincaid_grade', 'avg_sentence_length', 'avg_word_length', 'type_token_ratio', 'long_word_percentage', 'complex_word_count', 'syllable_count', 'movie_release_year', 'review_year', 'years_since_release', 'review_window', 'total_votes', 'helpfulness_ratio', 'vote_polarization', 'has_engagement', 'paragraph_count', 'avg_paragraph_length', 'exclamation_count', 'question_mark_count', 'ellipsis_count', 'caps_word_count', 'quote_count', 'double_quote_count', 'single_quote_count', 'punctuation_density', 'uppercase_ratio', 'adj_ratio', 'verb_ratio', 'noun_ratio', 'adverb_ratio', 'pronoun_ratio', 'first_person_ratio', 'second_person_ratio', 'determiner_ratio', 'conjunction_ratio']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INCREMENTAL FEATURE EXTRACTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTotal reviews: {len(df):,}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Features Improved/Added:\")\n",
    "print(f\"  âœ… Gender Detection (v2): {(df['username_gender_hint'] != 'unknown').sum()} identified ({(df['username_gender_hint'] != 'unknown').sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "if not SKIP_EMOTIONS:\n",
    "    print(f\"  âœ… Emotion Detection (NEW): 8 columns added\")\n",
    "else:\n",
    "    print(f\"  â­ï¸  Emotion Detection: Already existed, skipped\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ Complete Feature List ({len(df.columns)} columns):\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Updated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXPORTING UPDATED DATASET\n",
      "============================================================\n",
      "\n",
      "âœ… Saved: /Users/jamesroot/Desktop/JAMES/Noetheca/Reviews/Data/reviews_enhanced.csv\n",
      "   Rows: 3,222\n",
      "   Columns: 76\n",
      "   File size: 5.55 MB\n",
      "\n",
      "============================================================\n",
      "âœ… INCREMENTAL UPDATES COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Ready for analysis phase (movie_insights.ipynb)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPORTING UPDATED DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8')\n",
    "print(f\"\\nâœ… Saved: {OUTPUT_FILE}\")\n",
    "print(f\"   Rows: {len(df):,}\")\n",
    "print(f\"   Columns: {len(df.columns)}\")\n",
    "\n",
    "# File size\n",
    "file_size = OUTPUT_FILE.stat().st_size / (1024 * 1024)  # MB\n",
    "print(f\"   File size: {file_size:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… INCREMENTAL UPDATES COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nReady for analysis phase (movie_insights.ipynb)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GENERATING HTML REPORT\n",
      "============================================================\n",
      "\n",
      "âœ… HTML report generated: /Users/jamesroot/Desktop/JAMES/Noetheca/Reviews/scripts/feature_extraction_incremental_report.html\n",
      "   Open in browser: file:///Users/jamesroot/Desktop/JAMES/Noetheca/Reviews/scripts/feature_extraction_incremental_report.html\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Generate HTML Report (Optional)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING HTML REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the notebook path\n",
    "notebook_path = Path('/Users/jamesroot/Desktop/JAMES/Noetheca/Reviews/scripts/feature_extraction_incremental.ipynb')\n",
    "output_path = notebook_path.parent / 'feature_extraction_incremental_report.html'\n",
    "\n",
    "try:\n",
    "    # Run nbconvert\n",
    "    result = subprocess.run([\n",
    "        'jupyter', 'nbconvert', \n",
    "        '--to', 'html',\n",
    "        '--no-input',\n",
    "        '--output', str(output_path),\n",
    "        str(notebook_path)\n",
    "    ], capture_output=True, text=True, check=True)\n",
    "    \n",
    "    print(f\"\\nâœ… HTML report generated: {output_path}\")\n",
    "    print(f\"   Open in browser: file://{output_path}\")\n",
    "    \n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"\\nâŒ Error generating HTML report:\")\n",
    "    print(f\"   {e.stderr}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"\\nâš ï¸  jupyter nbconvert not found. Install with:\")\n",
    "    print(\"   pip install nbconvert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
