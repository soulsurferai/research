{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92697da8-e121-4e66-8858-4d01f333d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 1: ENVIRONMENT SETUP & DEPENDENCIES\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import spacy\n",
    "\n",
    "# 1. LOAD ENVIRONMENT VARIABLES\n",
    "load_success = load_dotenv()\n",
    "if load_success:\n",
    "    print(f\"‚úÖ Environment variables loaded from .env\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Warning: .env file not found or empty.\")\n",
    "\n",
    "# 2. CONSTRUCT DATABASE CONNECTION\n",
    "# Fetch individual parts from your specific .env structure\n",
    "db_user = os.getenv(\"DB_USER\")\n",
    "db_pass = os.getenv(\"DB_PASS\")\n",
    "db_host = os.getenv(\"DB_HOST\")\n",
    "db_port = os.getenv(\"DB_PORT\")\n",
    "db_name = os.getenv(\"DB_NAME\")\n",
    "\n",
    "# Verify we have everything (except password, which might be empty for local dev sometimes)\n",
    "if not all([db_user, db_host, db_name]):\n",
    "    print(\"‚ùå Error: Missing DB_USER, DB_HOST, or DB_NAME in .env file.\")\n",
    "else:\n",
    "    # Build the SQLAlchemy connection string: postgresql://<user>:<pass>@<host>:<port>/<db>\n",
    "    # We use f-strings to assemble it dynamically\n",
    "    DB_STRING = f\"postgresql://{db_user}:{db_pass}@{db_host}:{db_port}/{db_name}\"\n",
    "    \n",
    "    try:\n",
    "        # Create Engine\n",
    "        engine = create_engine(DB_STRING)\n",
    "        \n",
    "        # Test Connection\n",
    "        with engine.connect() as conn:\n",
    "            print(f\"‚úÖ Database connection established to {db_host}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Database connection failed. Check your password in .env.\")\n",
    "        print(f\"   Error details: {e}\")\n",
    "\n",
    "# 3. SPACY MODEL CHECK\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"‚úÖ spaCy model 'en_core_web_sm' loaded.\")\n",
    "except OSError:\n",
    "    print(\"‚ùå spaCy model not found. Run 'python -m spacy download en_core_web_sm' in terminal.\")\n",
    "\n",
    "# 4. CONFIGURATION\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(\"üöÄ Setup Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4cf634-3c74-459e-b455-93a9307b68c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 2: BUILD FTS QUERY FROM LOTR ENTITIES JSON\n",
    "# ==============================================================================\n",
    "# Parse lotr_entities.json to construct a comprehensive tsquery string\n",
    "# This will be used to search the entire corpus via the content_tsv GIN index\n",
    "\n",
    "import json\n",
    "\n",
    "# Load the entity file\n",
    "entity_file = \"../lotr_entities.json\"\n",
    "\n",
    "with open(entity_file, 'r') as f:\n",
    "    lotr_data = json.load(f)\n",
    "\n",
    "# Extract all searchable terms (canonical names + aliases)\n",
    "all_terms = []\n",
    "\n",
    "def extract_terms(node):\n",
    "    \"\"\"Recursively extract canonical names and aliases from nested structure.\"\"\"\n",
    "    if isinstance(node, list):\n",
    "        for item in node:\n",
    "            if isinstance(item, dict) and \"canonical\" in item:\n",
    "                all_terms.append(item[\"canonical\"])\n",
    "                all_terms.extend(item.get(\"aliases\", []))\n",
    "    elif isinstance(node, dict):\n",
    "        for val in node.values():\n",
    "            extract_terms(val)\n",
    "\n",
    "extract_terms(lotr_data)\n",
    "\n",
    "print(f\"‚úÖ Extracted {len(all_terms):,} searchable terms from {entity_file}\")\n",
    "\n",
    "# Clean and deduplicate\n",
    "all_terms = list(set(all_terms))  # Remove duplicates\n",
    "all_terms = [t for t in all_terms if len(t) >= 2]  # Drop single characters\n",
    "print(f\"   After deduplication: {len(all_terms):,} unique terms\")\n",
    "\n",
    "# Build tsquery string\n",
    "# Escape single quotes for SQL, wrap multi-word phrases appropriately\n",
    "def term_to_tsquery(term):\n",
    "    \"\"\"Convert a term to tsquery format.\"\"\"\n",
    "    # Escape single quotes\n",
    "    escaped = term.replace(\"'\", \"''\")\n",
    "    # Multi-word terms need adjacency operator\n",
    "    words = escaped.split()\n",
    "    if len(words) > 1:\n",
    "        return \" <-> \".join(f\"'{w}'\" for w in words)\n",
    "    else:\n",
    "        return f\"'{escaped}'\"\n",
    "\n",
    "tsquery_parts = [term_to_tsquery(t) for t in all_terms]\n",
    "lotr_tsquery = \" | \".join(tsquery_parts)\n",
    "\n",
    "print(f\"   Built tsquery with {len(tsquery_parts):,} OR clauses\")\n",
    "print(f\"\\nüìã Sample terms: {all_terms[:20]}\")\n",
    "print(f\"\\nüîç Query preview (first 500 chars):\\n{lotr_tsquery[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace49c69-c3e8-4983-91aa-50ac117b31cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 2a: FILTER OUT LOW-PRECISION TERMS\n",
    "# ==============================================================================\n",
    "# Remove terms that are too generic or overlap heavily with non-LOTR contexts\n",
    "\n",
    "generic_terms = {\n",
    "    # Single common words\n",
    "    'Mark', 'Warg', 'Wargs', 'Orcs', 'Goblins', 'Uruks', 'Eagles', 'Dwarves', \n",
    "    'Elves', 'Men', 'Ring', 'Rings', 'Tower', 'King', 'Shadow', 'Fire', 'Light',\n",
    "    'Dark', 'White', 'Black', 'Grey', 'Gray', 'Gold', 'Silver', 'Iron', 'Stone',\n",
    "    'Dragon', 'Dragons', 'Troll', 'Trolls', 'Spider', 'Spiders',\n",
    "    \n",
    "    # Generic phrases\n",
    "    'The War', 'The Mouth', 'The Eye', 'The Ring', 'The Tower', 'The King',\n",
    "    'The Shadow', 'The Dark', 'The White', 'The Black', 'The Grey',\n",
    "    'The Fellowship', 'The Company',\n",
    "    \n",
    "    # Too short / ambiguous\n",
    "    'Sam', 'Tom', 'Bill', 'Ted', 'Bert', 'Fatty',\n",
    "}\n",
    "\n",
    "before_filter = len(all_terms)\n",
    "all_terms = [t for t in all_terms if t not in generic_terms]\n",
    "print(f\"üßπ Filtered out {before_filter - len(all_terms)} generic/ambiguous terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b437e95-7f1c-4aff-b191-83a807445d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 3: CORPUS-WIDE LOTR COMMENT PULL\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üîç PULLING LOTR MENTIONS CORPUS-WIDE...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "lotr_fts_query = \"\"\"\n",
    "    'Lord' <-> 'of' <-> 'the' <-> 'Rings' | Gandalf | Frodo | Aragorn | Mordor | \n",
    "    Tolkien | Silmarillion | Hobbiton | Rivendell | Gondor | Rohan | LOTR | LotR | \n",
    "    Sauron | Gollum | Legolas | Gimli | Boromir | Saruman | Hobbit | Bilbo | Moria | Isengard\n",
    "\"\"\"\n",
    "\n",
    "comments_sql = f\"\"\"\n",
    "    SELECT \n",
    "        rc.id as comment_id,\n",
    "        rc.post_id,\n",
    "        rc.subreddit,\n",
    "        rc.author,\n",
    "        rc.content,\n",
    "        rc.score,\n",
    "        rc.created_at,\n",
    "        uac.authenticity_score,\n",
    "        uac.subreddit_count as author_subreddit_count,\n",
    "        uac.active_days as author_active_days\n",
    "    FROM reddit_comments rc\n",
    "    LEFT JOIN user_authenticity_cache uac ON rc.author = uac.author\n",
    "    WHERE rc.content_tsv @@ to_tsquery('english', $${lotr_fts_query}$$)\n",
    "      AND rc.is_deleted = FALSE\n",
    "\"\"\"\n",
    "\n",
    "df_lotr_comments = pd.read_sql(comments_sql, engine)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df_lotr_comments):,} comments\")\n",
    "print(f\"   Unique subreddits: {df_lotr_comments['subreddit'].nunique():,}\")\n",
    "print(f\"   Unique authors: {df_lotr_comments['author'].nunique():,}\")\n",
    "print(f\"   Date range: {df_lotr_comments['created_at'].min()} to {df_lotr_comments['created_at'].max()}\")\n",
    "print(f\"\\nüìä Authenticity distribution:\")\n",
    "print(df_lotr_comments['authenticity_score'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b50be7d-f1fc-414e-a670-1dfe293328fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 4: CORPUS-WIDE LOTR POST PULL\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üîç PULLING LOTR POSTS CORPUS-WIDE...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "posts_sql = f\"\"\"\n",
    "    SELECT \n",
    "        rp.post_id,\n",
    "        rp.subreddit,\n",
    "        rp.author,\n",
    "        rp.title,\n",
    "        rp.content,\n",
    "        rp.url,\n",
    "        rp.score,\n",
    "        rp.num_comments,\n",
    "        rp.created_at,\n",
    "        uac.authenticity_score,\n",
    "        uac.subreddit_count as author_subreddit_count,\n",
    "        uac.active_days as author_active_days\n",
    "    FROM reddit_posts rp\n",
    "    LEFT JOIN user_authenticity_cache uac ON rp.author = uac.author\n",
    "    WHERE rp.content_tsv @@ to_tsquery('english', $${lotr_fts_query}$$)\n",
    "\"\"\"\n",
    "\n",
    "df_lotr_posts = pd.read_sql(posts_sql, engine)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df_lotr_posts):,} posts\")\n",
    "print(f\"   Unique subreddits: {df_lotr_posts['subreddit'].nunique():,}\")\n",
    "print(f\"   Unique authors: {df_lotr_posts['author'].nunique():,}\")\n",
    "print(f\"   Date range: {df_lotr_posts['created_at'].min()} to {df_lotr_posts['created_at'].max()}\")\n",
    "print(f\"\\nüìä Authenticity distribution:\")\n",
    "print(df_lotr_posts['authenticity_score'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257fb8f8-c255-49bf-9c42-02621326da65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 5: MERGE POST CONTEXT ONTO COMMENTS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üîó MERGING POST CONTEXT ONTO COMMENTS...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a lookup of post_id -> title\n",
    "post_titles = df_lotr_posts[['post_id', 'title']].drop_duplicates()\n",
    "post_titles = post_titles.rename(columns={'title': 'post_title'})\n",
    "\n",
    "# Merge onto comments\n",
    "df_lotr_comments = df_lotr_comments.merge(post_titles, on='post_id', how='left')\n",
    "\n",
    "# Check coverage\n",
    "has_title = df_lotr_comments['post_title'].notna().sum()\n",
    "missing_title = df_lotr_comments['post_title'].isna().sum()\n",
    "\n",
    "print(f\"‚úÖ Comments with post title: {has_title:,} ({has_title/len(df_lotr_comments)*100:.1f}%)\")\n",
    "print(f\"   Comments missing post title: {missing_title:,} ({missing_title/len(df_lotr_comments)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a600fe09-89f6-414a-9c42-89ec7f261cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 5b: FETCH MISSING POST TITLES\n",
    "# ==============================================================================\n",
    "\n",
    "# Get post_ids that are missing titles\n",
    "missing_post_ids = df_lotr_comments[df_lotr_comments['post_title'].isna()]['post_id'].unique()\n",
    "print(f\"üîç Fetching titles for {len(missing_post_ids):,} posts...\")\n",
    "\n",
    "# Batch fetch from reddit_posts\n",
    "missing_ids_str = \"','\".join(missing_post_ids)\n",
    "\n",
    "missing_titles_sql = f\"\"\"\n",
    "    SELECT post_id, title\n",
    "    FROM reddit_posts\n",
    "    WHERE post_id IN ('{missing_ids_str}')\n",
    "\"\"\"\n",
    "\n",
    "df_missing_titles = pd.read_sql(missing_titles_sql, engine)\n",
    "print(f\"   Found {len(df_missing_titles):,} titles\")\n",
    "\n",
    "# Update the main dataframe\n",
    "title_lookup = dict(zip(df_missing_titles['post_id'], df_missing_titles['title']))\n",
    "mask = df_lotr_comments['post_title'].isna()\n",
    "df_lotr_comments.loc[mask, 'post_title'] = df_lotr_comments.loc[mask, 'post_id'].map(title_lookup)\n",
    "\n",
    "# Final coverage\n",
    "has_title = df_lotr_comments['post_title'].notna().sum()\n",
    "print(f\"\\n‚úÖ Final coverage: {has_title:,} / {len(df_lotr_comments):,} ({has_title/len(df_lotr_comments)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a98d505-ef7f-4296-af6c-dd75cc994396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 6: SUBREDDIT DISTRIBUTION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üìä LOTR MENTIONS BY SUBREDDIT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Comments by subreddit\n",
    "sub_comments = df_lotr_comments.groupby('subreddit').agg(\n",
    "    comment_count=('comment_id', 'count'),\n",
    "    unique_authors=('author', 'nunique'),\n",
    "    avg_score=('score', 'mean'),\n",
    "    high_auth_pct=('authenticity_score', lambda x: (x == 'HIGH').sum() / len(x) * 100)\n",
    ").round(1)\n",
    "\n",
    "sub_comments = sub_comments.sort_values('comment_count', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 30 subreddits by LOTR comment volume:\\n\")\n",
    "print(sub_comments.head(30).to_string())\n",
    "\n",
    "# How much is concentrated in dedicated LOTR subs?\n",
    "lotr_dedicated = ['lotr', 'tolkienfans', 'LOTR_on_Prime', 'lordoftherings', \n",
    "                  'Rings_Of_Power', 'lotro', 'lotrlcg', 'lotrmemes', \n",
    "                  'TheWarOfTheRohirrim', 'Silmarillionmemes']\n",
    "\n",
    "dedicated_count = df_lotr_comments[df_lotr_comments['subreddit'].isin(lotr_dedicated)]['comment_id'].count()\n",
    "other_count = len(df_lotr_comments) - dedicated_count\n",
    "\n",
    "print(f\"\\nüìç CONCENTRATION:\")\n",
    "print(f\"   Dedicated LOTR subs: {dedicated_count:,} ({dedicated_count/len(df_lotr_comments)*100:.1f}%)\")\n",
    "print(f\"   Other communities: {other_count:,} ({other_count/len(df_lotr_comments)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38206d7-4351-405e-a85b-fbc02895b91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 7: EXTERNAL COMMUNITIES - LOTR AS CULTURAL REFERENCE\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üåç LOTR MENTIONS IN NON-DEDICATED COMMUNITIES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Updated dedicated list\n",
    "lotr_dedicated = ['lotr', 'tolkienfans', 'LOTR_on_Prime', 'lordoftherings', \n",
    "                  'Rings_Of_Power', 'lotro', 'lotrlcg', 'lotrmemes', \n",
    "                  'TheWarOfTheRohirrim', 'Silmarillionmemes', 'TalesofTheShiregamers']\n",
    "\n",
    "# Filter to external only\n",
    "df_lotr_external = df_lotr_comments[~df_lotr_comments['subreddit'].isin(lotr_dedicated)]\n",
    "\n",
    "print(f\"Total external comments: {len(df_lotr_external):,}\")\n",
    "print(f\"Unique subreddits: {df_lotr_external['subreddit'].nunique()}\")\n",
    "print(f\"Unique authors: {df_lotr_external['author'].nunique():,}\")\n",
    "\n",
    "# Full list by volume\n",
    "external_subs = df_lotr_external.groupby('subreddit').agg(\n",
    "    comment_count=('comment_id', 'count'),\n",
    "    unique_authors=('author', 'nunique'),\n",
    "    avg_score=('score', 'mean')\n",
    ").round(1).sort_values('comment_count', ascending=False)\n",
    "\n",
    "print(f\"\\nüìã ALL {len(external_subs)} EXTERNAL SUBREDDITS:\\n\")\n",
    "print(external_subs.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efe04e0-a625-4d68-88b3-a252bddac86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 8: SAMPLE COMMENTS FROM TOP EXTERNAL COMMUNITIES\n",
    "# ==============================================================================\n",
    "\n",
    "# Top 15 external subreddits by volume\n",
    "top_external = external_subs.head(15).index.tolist()\n",
    "\n",
    "for sub in top_external:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"r/{sub} ({external_subs.loc[sub, 'comment_count']} comments)\")\n",
    "    print('='*60)\n",
    "    \n",
    "    samples = df_lotr_external[df_lotr_external['subreddit'] == sub].nlargest(3, 'score')\n",
    "    \n",
    "    for _, row in samples.iterrows():\n",
    "        print(f\"\\n[score: {row['score']}]\")\n",
    "        print(f\"{row['content'][:400]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fb411f-9e34-4611-99bd-85f34b1aca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 9: TALES OF THE SHIRE DEEP DIVE\n",
    "# ==============================================================================\n",
    "\n",
    "df_tales = df_lotr_comments[df_lotr_comments['subreddit'] == 'TalesofTheShiregamers']\n",
    "\n",
    "print(f\"üìä r/TalesofTheShiregamers\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"Total comments: {len(df_tales):,}\")\n",
    "print(f\"Unique authors: {df_tales['author'].nunique()}\")\n",
    "print(f\"Date range: {df_tales['created_at'].min().date()} to {df_tales['created_at'].max().date()}\")\n",
    "print(f\"Avg score: {df_tales['score'].mean():.1f}\")\n",
    "\n",
    "print(f\"\\nüìà Score distribution:\")\n",
    "print(df_tales['score'].describe())\n",
    "\n",
    "print(f\"\\nüìù Top 20 comments by score:\\n\")\n",
    "for _, row in df_tales.nlargest(20, 'score').iterrows():\n",
    "    print(f\"[{row['score']}] u/{row['author']} | {row['created_at'].date()}\")\n",
    "    print(f\"Post: {row['post_title'][:80] if pd.notna(row['post_title']) else 'N/A'}...\")\n",
    "    print(f\"{row['content'][:300]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d23e3e-74b0-44ac-b0c4-851dff1202ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 10: TALES OF THE SHIRE - FULL COMMENT CORPUS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üè° PULLING ALL r/TalesofTheShiregamers COMMENTS WITH POST CONTEXT...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "tales_full_sql = \"\"\"\n",
    "    SELECT \n",
    "        rc.id as comment_id,\n",
    "        rc.post_id,\n",
    "        rc.author,\n",
    "        rc.content,\n",
    "        rc.score,\n",
    "        rc.created_at,\n",
    "        rp.title as post_title,\n",
    "        rp.content as post_content,\n",
    "        rp.score as post_score,\n",
    "        rp.num_comments as post_num_comments\n",
    "    FROM reddit_comments rc\n",
    "    LEFT JOIN reddit_posts rp ON rc.post_id = rp.post_id\n",
    "    WHERE rc.subreddit = 'TalesofTheShiregamers'\n",
    "      AND rc.is_deleted = FALSE\n",
    "\"\"\"\n",
    "\n",
    "df_tales_full = pd.read_sql(tales_full_sql, engine)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df_tales_full):,} comments\")\n",
    "print(f\"   Unique posts: {df_tales_full['post_id'].nunique()}\")\n",
    "print(f\"   Unique authors: {df_tales_full['author'].nunique()}\")\n",
    "print(f\"   Date range: {df_tales_full['created_at'].min().date()} to {df_tales_full['created_at'].max().date()}\")\n",
    "\n",
    "print(f\"\\nüìä Top 15 posts by comment count:\\n\")\n",
    "top_posts = df_tales_full.groupby(['post_id', 'post_title']).agg(\n",
    "    comment_count=('comment_id', 'count'),\n",
    "    post_score=('post_score', 'first')\n",
    ").sort_values('comment_count', ascending=False).head(15)\n",
    "\n",
    "for (post_id, title), row in top_posts.iterrows():\n",
    "    print(f\"[{row['comment_count']} comments | score {row['post_score']}] {title[:70]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4635b1-f6fc-4faa-adc7-35648b85f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 11: TALES OF THE SHIRE AUTHOR CROSS-PARTICIPATION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üîç WHERE ELSE DO TotS CONTRIBUTORS PARTICIPATE?\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get all TotS authors\n",
    "tots_authors = df_tales_full['author'].unique().tolist()\n",
    "print(f\"TotS unique authors: {len(tots_authors):,}\")\n",
    "\n",
    "# Find all their comments across the corpus\n",
    "authors_str = \"','\".join([a.replace(\"'\", \"''\") for a in tots_authors])\n",
    "\n",
    "cross_participation_sql = f\"\"\"\n",
    "    SELECT \n",
    "        subreddit,\n",
    "        COUNT(*) as comment_count,\n",
    "        COUNT(DISTINCT author) as author_count\n",
    "    FROM reddit_comments\n",
    "    WHERE author IN ('{authors_str}')\n",
    "      AND is_deleted = FALSE\n",
    "    GROUP BY subreddit\n",
    "    ORDER BY author_count DESC\n",
    "\"\"\"\n",
    "\n",
    "df_cross = pd.read_sql(cross_participation_sql, engine)\n",
    "\n",
    "# Add percentage of TotS authors who participate in each sub\n",
    "df_cross['pct_of_tots_authors'] = (df_cross['author_count'] / len(tots_authors) * 100).round(1)\n",
    "\n",
    "print(f\"\\n‚úÖ TotS authors appear in {len(df_cross):,} subreddits\")\n",
    "print(f\"\\nüìä Top 40 subreddits by TotS author overlap:\\n\")\n",
    "print(df_cross.head(40).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e005fe-83c0-4656-b331-ab75e95bb5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 12: TOTS AUTHORS IN r/CozyGamers\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üéÆ WHAT DO TotS AUTHORS SAY IN r/CozyGamers?\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get TotS authors' comments in CozyGamers\n",
    "tots_authors_str = \"','\".join([a.replace(\"'\", \"''\") for a in tots_authors])\n",
    "\n",
    "cozygamers_sql = f\"\"\"\n",
    "    SELECT \n",
    "        rc.id as comment_id,\n",
    "        rc.post_id,\n",
    "        rc.author,\n",
    "        rc.content,\n",
    "        rc.score,\n",
    "        rc.created_at,\n",
    "        rp.title as post_title\n",
    "    FROM reddit_comments rc\n",
    "    LEFT JOIN reddit_posts rp ON rc.post_id = rp.post_id\n",
    "    WHERE rc.author IN ('{tots_authors_str}')\n",
    "      AND rc.subreddit = 'CozyGamers'\n",
    "      AND rc.is_deleted = FALSE\n",
    "    ORDER BY rc.score DESC\n",
    "\"\"\"\n",
    "\n",
    "df_tots_cozy = pd.read_sql(cozygamers_sql, engine)\n",
    "\n",
    "print(f\"‚úÖ {len(df_tots_cozy):,} comments from {df_tots_cozy['author'].nunique()} TotS authors\")\n",
    "print(f\"   Date range: {df_tots_cozy['created_at'].min().date()} to {df_tots_cozy['created_at'].max().date()}\")\n",
    "\n",
    "print(f\"\\nüìù Top 25 comments by score:\\n\")\n",
    "for _, row in df_tots_cozy.head(25).iterrows():\n",
    "    print(f\"[{row['score']}] u/{row['author']} | {row['created_at'].date()}\")\n",
    "    print(f\"Post: {row['post_title'][:70] if pd.notna(row['post_title']) else 'N/A'}...\")\n",
    "    print(f\"{row['content'][:250]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1711463-0250-4458-977e-ffaa3b95e703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 13: GAME MENTIONS BY TOTS AUTHORS IN r/CozyGamers\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üéÆ GAMES MENTIONED BY TotS AUTHORS IN r/CozyGamers\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define games to search for (common cozy games)\n",
    "cozy_games = [\n",
    "    'Animal Crossing', 'Stardew Valley', 'Disney Dreamlight Valley', 'Dreamlight Valley',\n",
    "    'Coral Island', 'Fae Farm', 'Rune Factory', 'Story of Seasons', 'Harvest Moon',\n",
    "    'Wylde Flowers', 'Cat Cafe Manager', 'Luma Island', 'Spiritfarer', 'Cozy Grove',\n",
    "    'Sun Haven', 'Ooblets', 'Slime Rancher', 'Unpacking', 'A Short Hike',\n",
    "    'Moonstone Island', 'Palia', 'Roots of Pacha', 'Dinkum', 'Portia', 'Sandrock',\n",
    "    'Fantasy Life', 'Littlewood', 'Garden Story', 'Witchbrook', 'Haunted Chocolatier',\n",
    "    'Tales of the Shire', 'Tales from the Shire', 'TotS', 'Fields of Mistria',\n",
    "    'Calico', 'Bear and Breakfast', 'Snacko', 'Mineko', 'Yokai Inn', 'Echoes of the Plum Grove',\n",
    "    'Sims', 'My Time at Portia', 'My Time at Sandrock', 'Everholm', 'Travellers Rest'\n",
    "]\n",
    "\n",
    "# Count mentions\n",
    "game_counts = {}\n",
    "for game in cozy_games:\n",
    "    count = df_tots_cozy['content'].str.contains(game, case=False, na=False).sum()\n",
    "    if count > 0:\n",
    "        game_counts[game] = count\n",
    "\n",
    "# Sort by frequency\n",
    "game_counts_sorted = dict(sorted(game_counts.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "print(f\"\\nüìä Game mentions (in {len(df_tots_cozy)} comments):\\n\")\n",
    "for game, count in game_counts_sorted.items():\n",
    "    pct = count / len(df_tots_cozy) * 100\n",
    "    print(f\"{game:30} {count:4} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed8793-3929-48a4-9aef-bf77df2db25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 14: CHECK NLP LIBRARIES\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üîç CHECKING NLP LIBRARIES...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "libraries = {\n",
    "    'transformers': None,\n",
    "    'torch': None,\n",
    "    'textstat': None,\n",
    "    'scipy': None,\n",
    "    'sklearn': None\n",
    "}\n",
    "\n",
    "for lib in libraries:\n",
    "    try:\n",
    "        module = __import__(lib)\n",
    "        version = getattr(module, '__version__', 'installed')\n",
    "        libraries[lib] = version\n",
    "        print(f\"‚úÖ {lib}: {version}\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ùå {lib}: NOT INSTALLED\")\n",
    "\n",
    "print(\"\\nüìã Summary:\")\n",
    "missing = [lib for lib, ver in libraries.items() if ver is None]\n",
    "if missing:\n",
    "    print(f\"   Missing: {', '.join(missing)}\")\n",
    "    print(f\"   Install with: pip install {' '.join(missing)}\")\n",
    "else:\n",
    "    print(\"   All libraries available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d703b5-4f44-483f-9207-ae0a226a626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 14b: CHECK NLP MODELS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üîç CHECKING NLP MODELS...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Test GoEmotions (Reddit-trained emotion detection)\n",
    "print(\"\\n1. GoEmotions (Reddit-trained, 27 emotions)...\")\n",
    "try:\n",
    "    emotion_classifier = pipeline(\"text-classification\", \n",
    "                                  model=\"SamLowe/roberta-base-go_emotions\", \n",
    "                                  top_k=5)\n",
    "    test = emotion_classifier(\"This game rocks! I love it!\")\n",
    "    print(f\"   ‚úÖ Loaded successfully\")\n",
    "    print(f\"   Test output: {test[0][:3]}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Failed: {e}\")\n",
    "\n",
    "# Test sentiment (social media trained)\n",
    "print(\"\\n2. Twitter-RoBERTa Sentiment...\")\n",
    "try:\n",
    "    sentiment_classifier = pipeline(\"sentiment-analysis\",\n",
    "                                    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "    test = sentiment_classifier(\"This game rocks! I love it!\")\n",
    "    print(f\"   ‚úÖ Loaded successfully\")\n",
    "    print(f\"   Test output: {test}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Failed: {e}\")\n",
    "\n",
    "# Test textstat\n",
    "print(\"\\n3. Textstat (reading level)...\")\n",
    "try:\n",
    "    import textstat\n",
    "    test_text = \"The hobbits of the Shire enjoy a peaceful life of farming and good food.\"\n",
    "    fk = textstat.flesch_kincaid_grade(test_text)\n",
    "    print(f\"   ‚úÖ Loaded successfully\")\n",
    "    print(f\"   Test Flesch-Kincaid grade: {fk}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Failed: {e}\")\n",
    "\n",
    "print(\"\\nüìã Ready to process!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b3d775-befa-48f4-904d-3821af6937b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 14c: TRY ALTERNATIVE SENTIMENT MODELS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üîç TRYING ALTERNATIVE SENTIMENT MODELS...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Option 1: distilbert sentiment (very reliable)\n",
    "print(\"\\n1. DistilBERT Sentiment...\")\n",
    "try:\n",
    "    sentiment_classifier = pipeline(\"sentiment-analysis\",\n",
    "                                    model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "    test = sentiment_classifier(\"This game rocks! I love it!\")\n",
    "    print(f\"   ‚úÖ Loaded successfully\")\n",
    "    print(f\"   Test output: {test}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Failed: {e}\")\n",
    "\n",
    "# Option 2: nlptown 5-star sentiment (more granular)\n",
    "print(\"\\n2. NLPTown 5-star Sentiment...\")\n",
    "try:\n",
    "    sentiment_5star = pipeline(\"sentiment-analysis\",\n",
    "                               model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "    test = sentiment_5star(\"This game rocks! I love it!\")\n",
    "    print(f\"   ‚úÖ Loaded successfully\")\n",
    "    print(f\"   Test output: {test}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Failed: {e}\")\n",
    "\n",
    "# Option 3: cardiffnlp older version\n",
    "print(\"\\n3. Cardiff Twitter Sentiment (older)...\")\n",
    "try:\n",
    "    sentiment_twitter = pipeline(\"sentiment-analysis\",\n",
    "                                 model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "    test = sentiment_twitter(\"This game rocks! I love it!\")\n",
    "    print(f\"   ‚úÖ Loaded successfully\")\n",
    "    print(f\"   Test output: {test}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e257518-72cb-4a15-a1c4-58d94e4f6683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 15: NLP ANALYSIS ON TALES OF THE SHIRE\n",
    "# ==============================================================================\n",
    "\n",
    "import textstat\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"üß† RUNNING NLP ANALYSIS ON TotS COMMENTS...\")\n",
    "print(f\"   Processing {len(df_tales_full):,} comments\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Combine post_title + content for context\n",
    "df_tales_full['full_text'] = df_tales_full.apply(\n",
    "    lambda row: f\"Post: {row['post_title']} Comment: {row['content']}\" \n",
    "    if pd.notna(row['post_title']) else row['content'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Truncate to 512 tokens (model limit)\n",
    "df_tales_full['full_text_truncated'] = df_tales_full['full_text'].str[:512]\n",
    "\n",
    "# Initialize results\n",
    "sentiments = []\n",
    "emotions = []\n",
    "reading_levels = []\n",
    "\n",
    "# Process in batches\n",
    "print(\"\\n‚è≥ Processing (this may take a few minutes)...\")\n",
    "\n",
    "for idx, row in tqdm(df_tales_full.iterrows(), total=len(df_tales_full)):\n",
    "    text = row['full_text_truncated']\n",
    "    \n",
    "    # Sentiment (5-star)\n",
    "    try:\n",
    "        sent = sentiment_5star(text)[0]\n",
    "        sentiments.append({'label': sent['label'], 'score': sent['score']})\n",
    "    except:\n",
    "        sentiments.append({'label': None, 'score': None})\n",
    "    \n",
    "    # Emotion (top 3)\n",
    "    try:\n",
    "        emo = emotion_classifier(text)[0][:3]\n",
    "        emotions.append(emo)\n",
    "    except:\n",
    "        emotions.append(None)\n",
    "    \n",
    "    # Reading level (on comment only, not combined)\n",
    "    try:\n",
    "        rl = textstat.flesch_kincaid_grade(row['content'])\n",
    "        reading_levels.append(rl)\n",
    "    except:\n",
    "        reading_levels.append(None)\n",
    "\n",
    "# Add to dataframe\n",
    "df_tales_full['sentiment_label'] = [s['label'] for s in sentiments]\n",
    "df_tales_full['sentiment_score'] = [s['score'] for s in sentiments]\n",
    "df_tales_full['emotions'] = emotions\n",
    "df_tales_full['reading_level'] = reading_levels\n",
    "\n",
    "print(\"\\n‚úÖ NLP analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bdb674-05ee-4e7c-8e4c-47beb2d5f8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 16: TALES OF THE SHIRE NLP SUMMARY\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üìä TALES OF THE SHIRE NLP ANALYSIS RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Sentiment distribution\n",
    "print(\"\\nüé≠ SENTIMENT (5-star scale):\")\n",
    "print(df_tales_full['sentiment_label'].value_counts())\n",
    "\n",
    "# Average sentiment score\n",
    "print(f\"\\nAverage confidence: {df_tales_full['sentiment_score'].mean():.2f}\")\n",
    "\n",
    "# Reading level\n",
    "print(f\"\\nüìñ READING LEVEL (Flesch-Kincaid Grade):\")\n",
    "print(f\"   Mean: {df_tales_full['reading_level'].mean():.1f}\")\n",
    "print(f\"   Median: {df_tales_full['reading_level'].median():.1f}\")\n",
    "print(f\"   Std: {df_tales_full['reading_level'].std():.1f}\")\n",
    "\n",
    "# Top emotions\n",
    "print(\"\\nüí´ TOP EMOTIONS (aggregated):\")\n",
    "emotion_counts = {}\n",
    "for emo_list in df_tales_full['emotions'].dropna():\n",
    "    for e in emo_list:\n",
    "        label = e['label']\n",
    "        emotion_counts[label] = emotion_counts.get(label, 0) + 1\n",
    "\n",
    "emotion_sorted = sorted(emotion_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "for emotion, count in emotion_sorted[:15]:\n",
    "    pct = count / len(df_tales_full) * 100\n",
    "    print(f\"   {emotion:20} {count:5} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f442ee8-91bf-4f47-b349-9abbfb46c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 17: WHAT'S DRIVING NEGATIVE EMOTIONS?\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üîç COMMENTS WITH HIGH CONFUSION/DISAPPOINTMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Find comments where confusion or disappointment was top emotion\n",
    "def get_top_emotion(emo_list):\n",
    "    if emo_list and len(emo_list) > 0:\n",
    "        return emo_list[0]['label']\n",
    "    return None\n",
    "\n",
    "df_tales_full['top_emotion'] = df_tales_full['emotions'].apply(get_top_emotion)\n",
    "\n",
    "# Confusion drivers\n",
    "print(\"\\nüòï TOP CONFUSION COMMENTS:\\n\")\n",
    "confused = df_tales_full[df_tales_full['top_emotion'] == 'confusion'].nlargest(10, 'score')\n",
    "for _, row in confused.iterrows():\n",
    "    print(f\"[score {row['score']}] Post: {row['post_title'][:50]}...\")\n",
    "    print(f\"   {row['content'][:200]}...\")\n",
    "    print()\n",
    "\n",
    "# Disappointment drivers\n",
    "print(\"\\nüòû TOP DISAPPOINTMENT COMMENTS:\\n\")\n",
    "disappointed = df_tales_full[df_tales_full['top_emotion'] == 'disappointment'].nlargest(10, 'score')\n",
    "for _, row in disappointed.iterrows():\n",
    "    print(f\"[score {row['score']}] Post: {row['post_title'][:50]}...\")\n",
    "    print(f\"   {row['content'][:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc67af00-d577-480d-889d-a01a43ab9e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 18: TOTS FEATURE REQUESTS / WISHLIST\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üìù TALES OF THE SHIRE WISHLIST ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Find wishlist-type posts\n",
    "wishlist_keywords = ['feature', 'want', 'wish', 'add', 'should', 'could', 'expand', 'missing', 'need', 'hope']\n",
    "pattern = '|'.join(wishlist_keywords)\n",
    "\n",
    "wishlist_posts = df_tales_full[df_tales_full['post_title'].str.contains(pattern, case=False, na=False)]['post_title'].unique()\n",
    "\n",
    "print(f\"Found {len(wishlist_posts)} potential wishlist posts:\\n\")\n",
    "for title in wishlist_posts[:30]:\n",
    "    count = len(df_tales_full[df_tales_full['post_title'] == title])\n",
    "    print(f\"[{count} comments] {title[:70]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25da0c28-fc89-4791-8c85-9f3b44dc795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 19: EXTRACT SPECIFIC FEATURE REQUESTS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üìù DETAILED WISHLIST EXTRACTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Key wishlist posts\n",
    "wishlist_titles = [\n",
    "    'What features would you like to see added?',\n",
    "    'Add-ons you think would improve the game even more!',\n",
    "    'Loving it!!! But I\\'m missing a few things',\n",
    "    'I wish you could sell meals...',\n",
    "    'I wish I could rearrange!!',\n",
    "    'Wish there was seasonal events',\n",
    "    'Have they said anything about adding multiplayer?',\n",
    "    'What could be expanded?'\n",
    "]\n",
    "\n",
    "df_wishlist = df_tales_full[df_tales_full['post_title'].isin(wishlist_titles)]\n",
    "print(f\"Wishlist comments: {len(df_wishlist)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "for title in wishlist_titles:\n",
    "    post_comments = df_tales_full[df_tales_full['post_title'] == title]\n",
    "    if len(post_comments) > 0:\n",
    "        print(f\"\\nüìå {title}\")\n",
    "        print(f\"   ({len(post_comments)} comments)\")\n",
    "        print(\"-\" * 50)\n",
    "        for _, row in post_comments.nlargest(5, 'score').iterrows():\n",
    "            print(f\"\\n   [{row['score']}] {row['content'][:250]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6626748f-1f38-4df6-9ce5-7ff452dbc034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 20: THEME VALIDATION BEYOND WISHLIST POSTS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üîç VALIDATING THEMES ACROSS FULL CORPUS (excluding wishlist posts)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Wishlist posts we already analyzed\n",
    "wishlist_titles = [\n",
    "    'What features would you like to see added?',\n",
    "    'Add-ons you think would improve the game even more!',\n",
    "    'Loving it!!! But I\\'m missing a few things',\n",
    "    'I wish you could sell meals...',\n",
    "    'I wish I could rearrange!!',\n",
    "    'Wish there was seasonal events',\n",
    "    'Have they said anything about adding multiplayer?',\n",
    "    'What could be expanded?'\n",
    "]\n",
    "\n",
    "# Exclude wishlist posts, require 100+ char comments\n",
    "df_non_wishlist = df_tales_full[\n",
    "    (~df_tales_full['post_title'].isin(wishlist_titles)) &\n",
    "    (df_tales_full['content'].str.len() >= 100)\n",
    "]\n",
    "\n",
    "print(f\"Corpus: {len(df_non_wishlist):,} comments (100+ chars, excluding wishlist posts)\")\n",
    "\n",
    "# Theme keywords\n",
    "themes = {\n",
    "    'furniture_sitting': r'\\b(sit|sitting|chair|bench|couch|furniture)\\b',\n",
    "    'animals': r'\\b(pet|petting|chicken|chickens|animal|animals|horse|duck)\\b',\n",
    "    'social_npc': r'\\b(invite|invited|dinner|lonely|npc|dialogue|gossip)\\b',\n",
    "    'birthday_events': r'\\b(birthday|party|festival|event|celebration)\\b',\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "for theme, pattern in themes.items():\n",
    "    matches = df_non_wishlist[\n",
    "        df_non_wishlist['content'].str.contains(pattern, case=False, na=False, regex=True)\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüìå {theme.upper()}: {len(matches)} mentions outside wishlist posts\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for _, row in matches.nlargest(5, 'score').iterrows():\n",
    "        print(f\"\\n[score {row['score']}] Post: {row['post_title'][:60]}...\")\n",
    "        print(f\"   {row['content'][:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039997de-e229-4097-8099-4715d3095cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 21: WHAT'S WORKING (PRAISE EXTRACTION)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üíö WHAT PLAYERS PRAISE (positive emotion comments)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Filter to comments where top emotion is positive\n",
    "positive_emotions = ['love', 'admiration', 'joy', 'gratitude']\n",
    "\n",
    "df_praise = df_tales_full[\n",
    "    (df_tales_full['top_emotion'].isin(positive_emotions)) &\n",
    "    (df_tales_full['content'].str.len() >= 100)\n",
    "]\n",
    "\n",
    "print(f\"Comments with positive top emotion (100+ chars): {len(df_praise):,}\")\n",
    "print(f\"\\nBreakdown:\")\n",
    "print(df_praise['top_emotion'].value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "for emotion in positive_emotions:\n",
    "    matches = df_praise[df_praise['top_emotion'] == emotion]\n",
    "    if len(matches) == 0:\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nüí´ {emotion.upper()}: {len(matches)} comments\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for _, row in matches.nlargest(5, 'score').iterrows():\n",
    "        print(f\"\\n[score {row['score']}] Post: {row['post_title'][:60]}...\")\n",
    "        print(f\"   {row['content'][:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0639520-8d6a-4ffa-bf50-fb9df18ceb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 22: TEMPORAL ANALYSIS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üìÖ TEMPORAL ANALYSIS: ARE THEMES PERSISTENT OR FADING?\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add week column\n",
    "df_tales_full['week'] = df_tales_full['created_at'].dt.to_period('W').apply(lambda x: x.start_time)\n",
    "\n",
    "# Overall volume by week\n",
    "weekly_volume = df_tales_full.groupby('week').size()\n",
    "\n",
    "print(f\"Date range: {df_tales_full['week'].min()} to {df_tales_full['week'].max()}\")\n",
    "print(f\"Total weeks: {len(weekly_volume)}\")\n",
    "\n",
    "# Theme keywords (same as Cell 20)\n",
    "themes = {\n",
    "    'furniture_sitting': r'\\b(sit|sitting|chair|bench|couch|furniture)\\b',\n",
    "    'animals': r'\\b(pet|petting|chicken|chickens|animal|animals|horse|duck)\\b',\n",
    "    'social_npc': r'\\b(invite|invited|dinner|lonely|npc|dialogue|gossip)\\b',\n",
    "    'birthday_events': r'\\b(birthday|party|festival|event|celebration)\\b',\n",
    "}\n",
    "\n",
    "# Count theme mentions by week\n",
    "theme_weekly = {}\n",
    "for theme, pattern in themes.items():\n",
    "    matches = df_tales_full[\n",
    "        df_tales_full['content'].str.contains(pattern, case=False, na=False, regex=True)\n",
    "    ]\n",
    "    theme_weekly[theme] = matches.groupby('week').size()\n",
    "\n",
    "# Create dataframe for plotting\n",
    "df_temporal = pd.DataFrame({\n",
    "    'total_volume': weekly_volume,\n",
    "    **theme_weekly\n",
    "}).fillna(0)\n",
    "\n",
    "# Calculate percentage of total (controls for volume fluctuation)\n",
    "for theme in themes.keys():\n",
    "    df_temporal[f'{theme}_pct'] = df_temporal[theme] / df_temporal['total_volume'] * 100\n",
    "\n",
    "print(\"\\nüìä WEEKLY THEME MENTIONS (as % of total comments):\\n\")\n",
    "print(df_temporal[[f'{t}_pct' for t in themes.keys()]].round(1).tail(15).to_string())\n",
    "\n",
    "# Summary stats\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìà THEME PERSISTENCE (avg % of weekly comments):\\n\")\n",
    "for theme in themes.keys():\n",
    "    col = f'{theme}_pct'\n",
    "    first_half = df_temporal[col].iloc[:len(df_temporal)//2].mean()\n",
    "    second_half = df_temporal[col].iloc[len(df_temporal)//2:].mean()\n",
    "    change = ((second_half - first_half) / first_half * 100) if first_half > 0 else 0\n",
    "    print(f\"   {theme:20} First half: {first_half:.1f}%  |  Second half: {second_half:.1f}%  |  Change: {change:+.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafb1cac-5cad-482b-80ee-4a0e4a9cbdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all DataFrame variables in memory\n",
    "import pandas as pd\n",
    "for name, obj in list(globals().items()):\n",
    "    if isinstance(obj, pd.DataFrame):\n",
    "        print(f\"{name}: {obj.shape[0]:,} rows, {obj.shape[1]} cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deec7bf7-4998-4954-b80f-37b23dba2f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('../output', exist_ok=True)\n",
    "\n",
    "df_lotr_comments.to_csv('../output/lotr_comments_69k.csv', index=False)\n",
    "df_lotr_posts.to_csv('../output/lotr_posts_5k.csv', index=False)\n",
    "df_wishlist.to_csv('../output/wishlist_signals.csv', index=False)\n",
    "df_praise.to_csv('../output/praise_signals.csv', index=False)\n",
    "df_lotr_external.to_csv('../output/external_sub_comments.csv', index=False)\n",
    "df_tales_full.to_csv('../output/tales_full.csv', index=False)\n",
    "df_temporal.to_csv('../output/temporal_analysis.csv', index=False)\n",
    "\n",
    "print(\"Done. Files saved to LOTR/output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710f3747-37c6-4baa-9d5b-86340d287601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
