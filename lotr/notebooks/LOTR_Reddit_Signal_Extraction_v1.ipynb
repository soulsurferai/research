{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92697da8-e121-4e66-8858-4d01f333d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 1: ENVIRONMENT SETUP & DEPENDENCIES\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import spacy\n",
    "\n",
    "# 1. LOAD ENVIRONMENT VARIABLES\n",
    "load_success = load_dotenv()\n",
    "if load_success:\n",
    "    print(f\"‚úÖ Environment variables loaded from .env\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Warning: .env file not found or empty.\")\n",
    "\n",
    "# 2. CONSTRUCT DATABASE CONNECTION\n",
    "# Fetch individual parts from your specific .env structure\n",
    "db_user = os.getenv(\"DB_USER\")\n",
    "db_pass = os.getenv(\"DB_PASS\")\n",
    "db_host = os.getenv(\"DB_HOST\")\n",
    "db_port = os.getenv(\"DB_PORT\")\n",
    "db_name = os.getenv(\"DB_NAME\")\n",
    "\n",
    "# Verify we have everything (except password, which might be empty for local dev sometimes)\n",
    "if not all([db_user, db_host, db_name]):\n",
    "    print(\"‚ùå Error: Missing DB_USER, DB_HOST, or DB_NAME in .env file.\")\n",
    "else:\n",
    "    # Build the SQLAlchemy connection string: postgresql://<user>:<pass>@<host>:<port>/<db>\n",
    "    # We use f-strings to assemble it dynamically\n",
    "    DB_STRING = f\"postgresql://{db_user}:{db_pass}@{db_host}:{db_port}/{db_name}\"\n",
    "    \n",
    "    try:\n",
    "        # Create Engine\n",
    "        engine = create_engine(DB_STRING)\n",
    "        \n",
    "        # Test Connection\n",
    "        with engine.connect() as conn:\n",
    "            print(f\"‚úÖ Database connection established to {db_host}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Database connection failed. Check your password in .env.\")\n",
    "        print(f\"   Error details: {e}\")\n",
    "\n",
    "# 3. SPACY MODEL CHECK\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"‚úÖ spaCy model 'en_core_web_sm' loaded.\")\n",
    "except OSError:\n",
    "    print(\"‚ùå spaCy model not found. Run 'python -m spacy download en_core_web_sm' in terminal.\")\n",
    "\n",
    "# 4. CONFIGURATION\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(\"üöÄ Setup Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4cf634-3c74-459e-b455-93a9307b68c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 2: LOAD KNOWLEDGE GRAPH (ENTITY RULER) - FIXED\n",
    "# ==============================================================================\n",
    "import json\n",
    "import os\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "# 1. SETUP ENTITY RULER\n",
    "# Remove old ruler if it exists to prevent duplicate rules\n",
    "if \"entity_ruler\" in nlp.pipe_names:\n",
    "    nlp.remove_pipe(\"entity_ruler\")\n",
    "    \n",
    "# Create new ruler and place it BEFORE the NER component\n",
    "# (This ensures \"The Shire\" is caught as a Location, not ignored)\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "\n",
    "# 2. DEFINE FILE MANIFEST\n",
    "entity_files = [\n",
    "    \"lotr_entities.json\",          # WHO (Lore)\n",
    "    \"vg_entities.json\",    # WHAT (Games)\n",
    "    \"vg_mechanics.json\",              # HOW (Mechanics)\n",
    "    \"vg_aesthetics.json\",             # VIBE (Aesthetics)\n",
    "    \"vg_tech.json\"                    # CONTEXT (Tech)\n",
    "]\n",
    "\n",
    "# 3. PARSE & LOAD PATTERNS\n",
    "patterns = []\n",
    "print(f\"--- LOADING KNOWLEDGE GRAPH ---\")\n",
    "\n",
    "for filename in entity_files:\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"‚ö†Ô∏è Warning: {filename} not found. Skipping.\")\n",
    "        continue\n",
    "        \n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    # A. GAMES (Array of objects with regex)\n",
    "    if \"games\" in data: \n",
    "        for game in data[\"games\"]:\n",
    "            # Use Regex if available, otherwise exact string\n",
    "            pat = [{\"TEXT\": {\"REGEX\": game[\"regex_pattern\"]}}] if \"regex_pattern\" in game else game[\"canonical_name\"]\n",
    "            patterns.append({\"label\": \"GAME\", \"pattern\": pat, \"id\": game[\"slug\"]})\n",
    "        print(f\"‚úÖ Loaded {len(data['games'])} Games from {filename}\")\n",
    "\n",
    "    # B. LOTR ENTITIES (Nested Dictionary)\n",
    "    elif \"characters\" in data: \n",
    "        # We use a mutable dict to track count, avoiding 'nonlocal' scope errors\n",
    "        stats = {\"count\": 0} \n",
    "        \n",
    "        def parse_lotr(node):\n",
    "            if isinstance(node, list):\n",
    "                for item in node:\n",
    "                    if isinstance(item, dict) and \"canonical\" in item:\n",
    "                        patterns.append({\"label\": \"LOTR_ENTITY\", \"pattern\": item[\"canonical\"], \"id\": item[\"canonical\"]})\n",
    "                        for alias in item.get(\"aliases\", []):\n",
    "                            patterns.append({\"label\": \"LOTR_ENTITY\", \"pattern\": alias, \"id\": item[\"canonical\"]})\n",
    "                        stats[\"count\"] += 1\n",
    "            elif isinstance(node, dict):\n",
    "                for val in node.values(): \n",
    "                    parse_lotr(val)\n",
    "                    \n",
    "        parse_lotr(data)\n",
    "        print(f\"‚úÖ Loaded ~{stats['count']} LOTR Entities from {filename}\")\n",
    "\n",
    "    # C. CONTEXT CONCEPTS (Mechanics/Tech/Aesthetics)\n",
    "    elif \"entities\" in data:\n",
    "        label = data.get(\"label\", \"CONCEPT\")\n",
    "        for item in data[\"entities\"]:\n",
    "            patterns.append({\"label\": label, \"pattern\": item[\"canonical\"], \"id\": item[\"canonical\"]})\n",
    "            for alias in item[\"aliases\"]:\n",
    "                patterns.append({\"label\": label, \"pattern\": alias, \"id\": item[\"canonical\"]})\n",
    "        print(f\"‚úÖ Loaded {len(data['entities'])} {label} concepts from {filename}\")\n",
    "\n",
    "# 4. COMMIT TO PIPELINE\n",
    "ruler.add_patterns(patterns)\n",
    "print(f\"\\nüöÄ PIPELINE READY: {len(patterns)} patterns active.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f107b06-e2dd-432e-b1e4-64676a10ce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 3: DATA INGESTION (VERIFIED HUMANS WITH POST CONTEXT)\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Stage 1: Pull comments from verified human authors\n",
    "print(\"‚è≥ Stage 1: Pulling comments from verified authors...\")\n",
    "\n",
    "comments_sql = \"\"\"\n",
    "SELECT \n",
    "    rc.id as comment_id,\n",
    "    rc.subreddit,\n",
    "    rc.author,\n",
    "    rc.content,\n",
    "    rc.score,\n",
    "    rc.created_at,\n",
    "    rc.post_id\n",
    "FROM reddit_comments rc\n",
    "JOIN user_authenticity_cache uac ON rc.author = uac.author\n",
    "WHERE rc.subreddit IN ('lotr','tolkienfans','LOTR_on_Prime','lordoftherings','Rings_Of_Power','lotro','TalesofTheShiregamers','lotrlcg','TheWarOfTheRohirrim','lotrmemes','Eldenring','skyrim','witcher','Witcher3','baldursgate','BG3','BaldursGate3','darksouls','darksouls3','Sekiro','fromsoftware','zelda','FinalFantasyVII','Genshin_Impact','reddeadredemption','wow','warcraftlore','ffxiv','Destiny','gaming','gamingsuggestions','videogames','GamingLeaksAndRumours','CozyGamers','CallOfDuty','apexlegends','leagueoflegends','VALORANT','pokemongo','FortNiteBR','Minecraft','GTA','StardewValley','VampireSurvivors','Warframe','ArcRaiders','southpark','Fantasy','Medievalart','bladerunner','FanFiction','movies','Music','TheSimpsons','cinematography','Filmmakers','writing')\n",
    "AND rc.is_deleted = FALSE\n",
    "AND uac.authenticity_score IN ('HIGH', 'MEDIUM');\n",
    "\"\"\"\n",
    "\n",
    "df_comments = pd.read_sql(comments_sql, engine)\n",
    "print(f\"‚úÖ Pulled {len(df_comments):,} comments\")\n",
    "print(f\"   Unique posts: {df_comments['post_id'].nunique():,}\")\n",
    "print(f\"   Unique authors: {df_comments['author'].nunique():,}\")\n",
    "print(f\"   Subreddits: {df_comments['subreddit'].nunique()}\")\n",
    "\n",
    "# Stage 2: Fetch post titles\n",
    "print(\"\\n‚è≥ Stage 2: Fetching post titles...\")\n",
    "\n",
    "post_ids = df_comments['post_id'].dropna().unique().tolist()\n",
    "print(f\"   Need titles for {len(post_ids):,} posts\")\n",
    "\n",
    "batch_size = 5000\n",
    "all_posts = []\n",
    "\n",
    "for i in range(0, len(post_ids), batch_size):\n",
    "    batch = post_ids[i:i+batch_size]\n",
    "    placeholders = ','.join([f\"'{p}'\" for p in batch])\n",
    "    posts_sql = f\"SELECT post_id, title FROM reddit_posts WHERE post_id IN ({placeholders})\"\n",
    "    df_batch = pd.read_sql(posts_sql, engine)\n",
    "    all_posts.append(df_batch)\n",
    "    print(f\"   Batch {i//batch_size + 1}: {len(df_batch):,} posts\")\n",
    "\n",
    "df_posts = pd.concat(all_posts, ignore_index=True) if all_posts else pd.DataFrame(columns=['post_id', 'title'])\n",
    "print(f\"‚úÖ Fetched {len(df_posts):,} post titles\")\n",
    "\n",
    "# Stage 3: Merge\n",
    "print(\"\\n‚è≥ Stage 3: Merging...\")\n",
    "df_raw = df_comments.merge(df_posts, on='post_id', how='left')\n",
    "df_raw.rename(columns={'title': 'post_title'}, inplace=True)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"‚úÖ FINAL DATASET: {len(df_raw):,} comments\")\n",
    "print(f\"   With post titles: {df_raw['post_title'].notna().sum():,}\")\n",
    "print(f\"   Unique authors: {df_raw['author'].nunique():,}\")\n",
    "print(f\"   Date range: {df_raw['created_at'].min()} to {df_raw['created_at'].max()}\")\n",
    "print(f\"\\nüìä Top subreddits:\")\n",
    "print(df_raw['subreddit'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f44897-730e-4b00-9f74-f27df78f5476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 3b: PULL ALL POSTS FOR CLASSIFICATION\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"‚è≥ Pulling posts from target subreddits...\")\n",
    "\n",
    "posts_sql = \"\"\"\n",
    "SELECT \n",
    "    post_id,\n",
    "    subreddit,\n",
    "    title,\n",
    "    content,\n",
    "    url,\n",
    "    score,\n",
    "    num_comments,\n",
    "    created_at\n",
    "FROM reddit_posts\n",
    "WHERE subreddit IN ('lotr','tolkienfans','LOTR_on_Prime','lordoftherings','Rings_Of_Power','lotro','TalesofTheShiregamers','lotrlcg','TheWarOfTheRohirrim','lotrmemes','Eldenring','skyrim','witcher','Witcher3','baldursgate','BG3','BaldursGate3','darksouls','darksouls3','Sekiro','fromsoftware','zelda','FinalFantasyVII','Genshin_Impact','reddeadredemption','wow','warcraftlore','ffxiv','Destiny','gaming','gamingsuggestions','videogames','GamingLeaksAndRumours','CozyGamers')\n",
    "ORDER BY score DESC;\n",
    "\"\"\"\n",
    "\n",
    "df_posts = pd.read_sql(posts_sql, engine)\n",
    "\n",
    "# Classify post type based on content\n",
    "df_posts['post_type'] = df_posts.apply(lambda row: \n",
    "    'TEXT' if pd.notna(row['content']) and len(str(row['content'])) > 10\n",
    "    else 'LINK' if pd.notna(row['url']) and 'reddit.com' not in str(row['url'])\n",
    "    else 'IMAGE' if pd.notna(row['url']) and any(x in str(row['url']) for x in ['i.redd.it', 'imgur', '.jpg', '.png', '.gif'])\n",
    "    else 'OTHER',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Pulled {len(df_posts):,} posts\")\n",
    "print(f\"\\nüìä By subreddit:\")\n",
    "print(df_posts['subreddit'].value_counts().head(15))\n",
    "print(f\"\\nüìä By post type:\")\n",
    "print(df_posts['post_type'].value_counts())\n",
    "print(f\"\\nüìä Sample titles:\")\n",
    "for _, row in df_posts.head(10).iterrows():\n",
    "    print(f\"  [{row['subreddit']}] {row['title'][:80]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbd99df-147f-470d-87ef-51393fb0062b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 3c: EXPLORE POST TITLES FOR INTENT PATTERNS\n",
    "# ==============================================================================\n",
    "\n",
    "# Look for question posts (most likely to have discussion value)\n",
    "question_posts = df_posts[df_posts['title'].str.contains(r'\\?', regex=True)]\n",
    "print(f\"üìä Posts with questions: {len(question_posts):,} ({len(question_posts)/len(df_posts)*100:.1f}%)\")\n",
    "\n",
    "# Sample question posts by subreddit type\n",
    "print(\"\\n--- LOTR SUBREDDIT QUESTIONS (sample) ---\")\n",
    "lotr_subs = ['lotr', 'tolkienfans', 'LOTR_on_Prime', 'lordoftherings', 'Rings_Of_Power', 'lotro']\n",
    "lotr_questions = question_posts[question_posts['subreddit'].isin(lotr_subs)].head(20)\n",
    "for _, row in lotr_questions.iterrows():\n",
    "    print(f\"  [{row['subreddit']}] {row['title'][:100]}\")\n",
    "\n",
    "print(\"\\n--- GAMING SUBREDDIT QUESTIONS (sample) ---\")\n",
    "gaming_subs = ['gaming', 'gamingsuggestions', 'videogames', 'Eldenring', 'BG3', 'skyrim', 'witcher']\n",
    "gaming_questions = question_posts[question_posts['subreddit'].isin(gaming_subs)].head(20)\n",
    "for _, row in gaming_questions.iterrows():\n",
    "    print(f\"  [{row['subreddit']}] {row['title'][:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da9040c-a575-4bf3-8aa9-fd359033a0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 3d: FIND HIGH-VALUE DISCUSSION POSTS\n",
    "# ==============================================================================\n",
    "\n",
    "# Define patterns that indicate valuable discussion types\n",
    "wishlist_patterns = r'(?i)(what would you want|dream game|wish they|should make|features you|would you like to see|what.+need|hope they|want.*sequel|want.*game)'\n",
    "pain_point_patterns = r'(?i)(worst thing|ruined|failed|why did.+fail|problem with|hate about|disappointed|what.+wrong|unpopular opinion|hot take)'\n",
    "comparison_patterns = r'(?i)(vs\\.?|versus|better than|compared to|best.+game|favorite.+game|top \\d|ranking|tier list)'\n",
    "recommendation_patterns = r'(?i)(games like|looking for|recommend|suggest|similar to|if you liked)'\n",
    "\n",
    "df_posts['is_wishlist'] = df_posts['title'].str.contains(wishlist_patterns, regex=True, na=False)\n",
    "df_posts['is_pain_point'] = df_posts['title'].str.contains(pain_point_patterns, regex=True, na=False)\n",
    "df_posts['is_comparison'] = df_posts['title'].str.contains(comparison_patterns, regex=True, na=False)\n",
    "df_posts['is_recommendation'] = df_posts['title'].str.contains(recommendation_patterns, regex=True, na=False)\n",
    "\n",
    "print(\"üìä HIGH-VALUE POST COUNTS:\")\n",
    "print(f\"   Wishlist posts: {df_posts['is_wishlist'].sum()}\")\n",
    "print(f\"   Pain point posts: {df_posts['is_pain_point'].sum()}\")\n",
    "print(f\"   Comparison posts: {df_posts['is_comparison'].sum()}\")\n",
    "print(f\"   Recommendation posts: {df_posts['is_recommendation'].sum()}\")\n",
    "\n",
    "print(\"\\n--- WISHLIST POSTS (sample) ---\")\n",
    "for _, row in df_posts[df_posts['is_wishlist']].head(15).iterrows():\n",
    "    print(f\"  [{row['subreddit']}] ({row['num_comments']} comments) {row['title'][:90]}\")\n",
    "\n",
    "print(\"\\n--- PAIN POINT POSTS (sample) ---\")\n",
    "for _, row in df_posts[df_posts['is_pain_point']].head(15).iterrows():\n",
    "    print(f\"  [{row['subreddit']}] ({row['num_comments']} comments) {row['title'][:90]}\")\n",
    "\n",
    "print(\"\\n--- COMPARISON POSTS (sample) ---\")\n",
    "for _, row in df_posts[df_posts['is_comparison']].head(15).iterrows():\n",
    "    print(f\"  [{row['subreddit']}] ({row['num_comments']} comments) {row['title'][:90]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c6b479-b7bf-4c97-913e-e9cd1ffae963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 3e: EXPLORE RECOMMENDATION POSTS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"--- RECOMMENDATION POSTS (sample) ---\")\n",
    "rec_posts = df_posts[df_posts['is_recommendation']].sort_values('num_comments', ascending=False)\n",
    "\n",
    "for _, row in rec_posts.head(30).iterrows():\n",
    "    print(f\"  [{row['subreddit']}] ({row['num_comments']} comments) {row['title'][:95]}\")\n",
    "\n",
    "print(f\"\\nüìä Recommendation posts by subreddit:\")\n",
    "print(rec_posts['subreddit'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db654ee-ced9-44c3-acc5-7bfafd1a9123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 3f: LOTR-SPECIFIC HIGH-VALUE POSTS\n",
    "# ==============================================================================\n",
    "\n",
    "lotr_subs = ['lotr', 'tolkienfans', 'LOTR_on_Prime', 'lordoftherings', 'Rings_Of_Power', 'lotro', 'lotrlcg', 'TheWarOfTheRohirrim', 'lotrmemes']\n",
    "\n",
    "lotr_posts = df_posts[df_posts['subreddit'].isin(lotr_subs)]\n",
    "\n",
    "# Any post mentioning games\n",
    "lotr_game_posts = lotr_posts[lotr_posts['title'].str.contains(r'(?i)(game|gaming|play|rpg|mmo|video)', regex=True, na=False)]\n",
    "\n",
    "print(f\"üìä LOTR subreddit posts mentioning games: {len(lotr_game_posts)}\")\n",
    "print(\"\\n--- LOTR + GAMES POSTS (sorted by comments) ---\")\n",
    "for _, row in lotr_game_posts.sort_values('num_comments', ascending=False).head(30).iterrows():\n",
    "    print(f\"  [{row['subreddit']}] ({row['num_comments']} comments) {row['title'][:95]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6440e803-e26c-4bd5-a31c-51bcc37e8a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 3g: PULL COMMENTS FROM HIGH-VALUE POSTS\n",
    "# ==============================================================================\n",
    "\n",
    "# Get post_ids for LOTR game discussion posts\n",
    "lotr_game_post_ids = lotr_game_posts.sort_values('num_comments', ascending=False).head(50)['post_id'].tolist()\n",
    "\n",
    "# Get post_ids for recommendation posts with high engagement\n",
    "rec_post_ids = df_posts[df_posts['is_recommendation']].sort_values('num_comments', ascending=False).head(100)['post_id'].tolist()\n",
    "\n",
    "# Get post_ids for comparison posts\n",
    "comparison_post_ids = df_posts[df_posts['is_comparison']].sort_values('num_comments', ascending=False).head(50)['post_id'].tolist()\n",
    "\n",
    "# Combine and dedupe\n",
    "high_value_post_ids = list(set(lotr_game_post_ids + rec_post_ids + comparison_post_ids))\n",
    "print(f\"üìä High-value posts identified: {len(high_value_post_ids)}\")\n",
    "\n",
    "# Pull comments from these posts\n",
    "print(\"\\n‚è≥ Pulling comments from high-value posts...\")\n",
    "\n",
    "placeholders = ','.join([f\"'{p}'\" for p in high_value_post_ids])\n",
    "comments_sql = f\"\"\"\n",
    "SELECT \n",
    "    rc.id as comment_id,\n",
    "    rc.subreddit,\n",
    "    rc.author,\n",
    "    rc.content,\n",
    "    rc.score,\n",
    "    rc.created_at,\n",
    "    rc.post_id\n",
    "FROM reddit_comments rc\n",
    "WHERE rc.post_id IN ({placeholders})\n",
    "AND rc.is_deleted = FALSE\n",
    "AND rc.author NOT IN ('[deleted]', 'AutoModerator', '[removed]');\n",
    "\"\"\"\n",
    "\n",
    "df_high_value_comments = pd.read_sql(comments_sql, engine)\n",
    "print(f\"‚úÖ Pulled {len(df_high_value_comments):,} comments from high-value posts\")\n",
    "\n",
    "# Merge with post info\n",
    "df_high_value = df_high_value_comments.merge(\n",
    "    df_posts[['post_id', 'title', 'subreddit']], \n",
    "    on='post_id', \n",
    "    how='left',\n",
    "    suffixes=('', '_post')\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Comments by source post type:\")\n",
    "print(f\"   From LOTR game posts: {df_high_value[df_high_value['post_id'].isin(lotr_game_post_ids)].shape[0]:,}\")\n",
    "print(f\"   From recommendation posts: {df_high_value[df_high_value['post_id'].isin(rec_post_ids)].shape[0]:,}\")\n",
    "print(f\"   From comparison posts: {df_high_value[df_high_value['post_id'].isin(comparison_post_ids)].shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f635c71e-6e3c-44d3-b13d-08096e69224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 3h: SAMPLE HIGH-VALUE COMMENTS\n",
    "# ==============================================================================\n",
    "\n",
    "# Tag comments with their post type\n",
    "df_high_value['post_type'] = 'other'\n",
    "df_high_value.loc[df_high_value['post_id'].isin(lotr_game_post_ids), 'post_type'] = 'lotr_game'\n",
    "df_high_value.loc[df_high_value['post_id'].isin(rec_post_ids), 'post_type'] = 'recommendation'\n",
    "df_high_value.loc[df_high_value['post_id'].isin(comparison_post_ids), 'post_type'] = 'comparison'\n",
    "\n",
    "print(\"--- LOTR GAME DISCUSSION COMMENTS (top scored) ---\\n\")\n",
    "lotr_sample = df_high_value[df_high_value['post_type'] == 'lotr_game'].nlargest(10, 'score')\n",
    "for _, row in lotr_sample.iterrows():\n",
    "    print(f\"[{row['score']} pts] Post: {row['title'][:60]}...\")\n",
    "    print(f\"   {row['content'][:200]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n--- RECOMMENDATION COMMENTS (top scored) ---\\n\")\n",
    "rec_sample = df_high_value[df_high_value['post_type'] == 'recommendation'].nlargest(10, 'score')\n",
    "for _, row in rec_sample.iterrows():\n",
    "    print(f\"[{row['score']} pts] Post: {row['title'][:60]}...\")\n",
    "    print(f\"   {row['content'][:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56b818b-04e6-400e-b4f5-02220ebfda38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 3i: LOOK AT ACTUAL LOTR GAME DISCUSSIONS\n",
    "# ==============================================================================\n",
    "\n",
    "# Find the specific high-value LOTR game posts\n",
    "lotr_game_titles = [\n",
    "    \"What is favorite LOTR video game\",\n",
    "    \"Imagine a Lord of the Rings game with the combat mechanics\",\n",
    "    \"Can we please get a proper AAA Lord of the Rings game\",\n",
    "    \"We need a LOTR game similar to Hogwarts Legacy\",\n",
    "    \"How do lord of the rings fans feel about this game\",\n",
    "    \"LoTR Video Game Discussion\",\n",
    "    \"Best game\"\n",
    "]\n",
    "\n",
    "# Find these posts\n",
    "target_posts = df_posts[df_posts['title'].str.contains('|'.join(lotr_game_titles), case=False, na=False)]\n",
    "print(f\"Found {len(target_posts)} target posts\\n\")\n",
    "\n",
    "for _, post in target_posts.iterrows():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"POST: {post['title'][:80]}\")\n",
    "    print(f\"Subreddit: {post['subreddit']} | Comments: {post['num_comments']} | Score: {post['score']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get comments for this post\n",
    "    post_comments = df_high_value[df_high_value['post_id'] == post['post_id']].nlargest(5, 'score')\n",
    "    \n",
    "    for _, c in post_comments.iterrows():\n",
    "        print(f\"\\n[{c['score']} pts] {c['content'][:300]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bfac75-8684-4186-b9ea-cc318924667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 3j: SYSTEMATIC THEME EXTRACTION WITH CITATIONS\n",
    "# ==============================================================================\n",
    "\n",
    "# Focus on the core LOTR game discussion posts\n",
    "core_lotr_game_posts = df_posts[df_posts['title'].str.contains(\n",
    "    'LOTR game|Lord of the Rings game|LOTR video game|favorite LOTR|Best game', \n",
    "    case=False, na=False\n",
    ") & df_posts['subreddit'].isin(['lotr', 'lordoftherings', 'tolkienfans', 'lotro'])]\n",
    "\n",
    "print(f\"üìä CORPUS STATISTICS\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"Total posts analyzed: {len(core_lotr_game_posts)}\")\n",
    "print(f\"Total comments in these posts: {df_high_value[df_high_value['post_id'].isin(core_lotr_game_posts['post_id'])].shape[0]}\")\n",
    "\n",
    "# Get all comments from these posts\n",
    "lotr_game_comments = df_high_value[df_high_value['post_id'].isin(core_lotr_game_posts['post_id'])].copy()\n",
    "lotr_game_comments = lotr_game_comments.merge(\n",
    "    core_lotr_game_posts[['post_id', 'title', 'num_comments', 'score']], \n",
    "    on='post_id', \n",
    "    suffixes=('', '_post')\n",
    ")\n",
    "\n",
    "print(f\"Unique authors: {lotr_game_comments['author'].nunique()}\")\n",
    "print(f\"Date range: {lotr_game_comments['created_at'].min().strftime('%Y-%m-%d')} to {lotr_game_comments['created_at'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"\\nSource posts:\")\n",
    "for _, p in core_lotr_game_posts.iterrows():\n",
    "    print(f\"  - \\\"{p['title'][:70]}\\\" ({p['num_comments']} comments, score: {p['score']})\")\n",
    "\n",
    "# Define theme detection patterns\n",
    "themes = {\n",
    "    'STRATEGY_GAMES': r'(?i)(strategy|rts|bfme|battle for middle earth|total war|grand strategy|4x)',\n",
    "    'OPEN_WORLD': r'(?i)(open world|hogwarts|skyrim|witcher|exploration|sandbox)',\n",
    "    'BOOK_VS_MOVIE': r'(?i)(book|tolkien|silmarillion|not.+movie|more than.+film|lore accurate|faithful)',\n",
    "    'NEMESIS_SYSTEM': r'(?i)(nemesis|shadow of (war|mordor))',\n",
    "    'LOTRO': r'(?i)(lotro|lord of the rings online|mmo)',\n",
    "    'NOSTALGIA': r'(?i)(ps2|return of the king|two towers|childhood|remember|nostalgia|classic)',\n",
    "    'AAA_QUALITY': r'(?i)(aaa|proper|real|actual|good.+game|quality)',\n",
    "    'SPECIFIC_MECHANICS': r'(?i)(combat|multiplayer|co-op|coop|online|pvp|crafting|skill tree)',\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üìä THEME ANALYSIS (n={len(lotr_game_comments)} comments)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for theme_name, pattern in themes.items():\n",
    "    matches = lotr_game_comments[lotr_game_comments['content'].str.contains(pattern, regex=True, na=False)]\n",
    "    \n",
    "    if len(matches) < 3:\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n### {theme_name}\")\n",
    "    print(f\"Mentions: {len(matches)} ({len(matches)/len(lotr_game_comments)*100:.1f}% of corpus)\")\n",
    "    print(f\"Unique authors: {matches['author'].nunique()}\")\n",
    "    print(f\"Avg score: {matches['score'].mean():.1f} (vs corpus avg: {lotr_game_comments['score'].mean():.1f})\")\n",
    "    \n",
    "    # Top quotes with full citation\n",
    "    print(f\"\\nTop-scored quotes:\")\n",
    "    for _, row in matches.nlargest(5, 'score').iterrows():\n",
    "        date = row['created_at'].strftime('%Y-%m-%d')\n",
    "        print(f\"\\n  [{row['score']} pts | {date} | r/{row['subreddit']} | u/{row['author']}]\")\n",
    "        print(f\"  Post: \\\"{row['title'][:60]}...\\\"\")\n",
    "        print(f\"  \\\"{row['content'][:250]}{'...' if len(row['content']) > 250 else ''}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3ccc77-24d9-4081-ad86-79d289029b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 4 (v2): CONTEXT-AWARE SIGNAL EXTRACTION\n",
    "# ==============================================================================\n",
    "from textblob import TextBlob \n",
    "\n",
    "# 1. DEFINE BUSINESS INTENT TRIGGERS\n",
    "INTENT_PATTERNS = {\n",
    "    \"WISHLIST\": [\"i wish\", \"i hope\", \"i want\", \"would be cool\", \"would love\", \"give us\", \"need a\", \"dream game\", \"if only\", \"should make\"],\n",
    "    \"PAIN_POINT\": [\"hate\", \"annoying\", \"broken\", \"sucks\", \"terrible\", \"worst\", \"boring\", \"glitch\", \"ruined\", \"disappointed\", \"trash\", \"clunky\", \"buggy\"],\n",
    "    \"NOSTALGIA\": [\"remember\", \"back in\", \"used to play\", \"childhood\", \"growing up\", \"nostalgia\", \"classic\", \"golden age\"],\n",
    "    \"HYPE\": [\"cant wait\", \"can't wait\", \"hyped\", \"excited\", \"looking forward\", \"day one\", \"pre-order\", \"preordered\"]\n",
    "}\n",
    "\n",
    "# 2. DEFINE NOISE FILTERS (The \"Context Validator\")\n",
    "# If a word is \"Ambiguous\", it MUST appear near a \"Validator\" word to count.\n",
    "AMBIGUOUS_TERMS = {\n",
    "    \"broken\": [\"game\", \"quest\", \"save\", \"mechanic\", \"glitch\", \"crash\", \"bug\", \"software\", \"code\", \"pc\", \"console\"],\n",
    "    \"pc\": [\"port\", \"release\", \"version\", \"edition\", \"steam\", \"windows\", \"run\"],\n",
    "    \"lore\": [\"break\", \"accurate\", \"canon\", \"book\", \"change\", \"source\"]\n",
    "}\n",
    "\n",
    "def validate_context(entity_text, sentence_text):\n",
    "    \"\"\"Returns False if the entity is used in a generic/irrelevant way.\"\"\"\n",
    "    text_lower = sentence_text.lower()\n",
    "    ent_lower = entity_text.lower()\n",
    "    \n",
    "    # If the entity is in our \"Ambiguous\" list, check for required context\n",
    "    for risky_term, required_context in AMBIGUOUS_TERMS.items():\n",
    "        if risky_term in ent_lower:\n",
    "            # It's a risky word. Does the sentence contain a validator?\n",
    "            if not any(ctx in text_lower for ctx in required_context):\n",
    "                return False # Risky word used without context (e.g., \"broken heart\")\n",
    "                \n",
    "    return True # Safe to keep\n",
    "\n",
    "def detect_intent(text):\n",
    "    text_lower = text.lower()\n",
    "    for intent, triggers in INTENT_PATTERNS.items():\n",
    "        if any(t in text_lower for t in triggers):\n",
    "            return intent\n",
    "    return \"OPINION\"\n",
    "\n",
    "def analyze_signals(dataframe):\n",
    "    results = []\n",
    "    print(f\"üöÄ Analyzing {len(dataframe)} comments (Context-Aware Mode)...\")\n",
    "    \n",
    "    docs = nlp.pipe(dataframe['content'].astype(str), batch_size=50)\n",
    "    \n",
    "    for doc, (index, row) in zip(docs, dataframe.iterrows()):\n",
    "        for sent in doc.sents:\n",
    "            found_ents = [e for e in sent.ents if e.label_ in ['GAME', 'MECHANIC', 'AESTHETIC', 'TECH']]\n",
    "            if not found_ents: continue\n",
    "            \n",
    "            # --- NEW STEP: CONTEXT CHECK ---\n",
    "            valid_ents = []\n",
    "            for ent in found_ents:\n",
    "                if validate_context(ent.text, sent.text):\n",
    "                    valid_ents.append(ent)\n",
    "            \n",
    "            if not valid_ents: continue\n",
    "            # -------------------------------\n",
    "\n",
    "            intent = detect_intent(sent.text)\n",
    "            sentiment = TextBlob(sent.text).sentiment.polarity\n",
    "            \n",
    "            for ent in valid_ents:\n",
    "                results.append({\n",
    "                    'comment_id': row['comment_id'],\n",
    "                    'subreddit': row['subreddit'],\n",
    "                    'author': row['author'],\n",
    "                    'score': row['score'],\n",
    "                    'entity': ent.text,\n",
    "                    'normalized_id': ent.ent_id_,\n",
    "                    'category': ent.label_,\n",
    "                    'intent': intent,\n",
    "                    'sentiment': sentiment,\n",
    "                    'context': sent.text\n",
    "                })\n",
    "                \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# RUN ANALYSIS\n",
    "if 'df_raw' in locals() and not df_raw.empty:\n",
    "    df_signals = analyze_signals(df_raw)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Extraction Complete: {len(df_signals)} High-Confidence Signals.\")\n",
    "    print(\"\\n--- üßû‚Äç‚ôÇÔ∏è TOP WISHLIST ITEMS (Refined) ---\")\n",
    "    print(df_signals[df_signals['intent'] == 'WISHLIST']['normalized_id'].value_counts().head(10))\n",
    "    print(\"\\n--- üö© TOP PAIN POINTS (Refined) ---\")\n",
    "    print(df_signals[df_signals['intent'] == 'PAIN_POINT']['normalized_id'].value_counts().head(10))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è df_raw is missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dfcdc2-48e1-4621-a679-c65281c66e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 5: THE \"GOLDEN QUOTE\" HUNTER (VERBATIMS FOR THE DECK)\n",
    "# ==============================================================================\n",
    "\n",
    "def get_golden_quotes(entity_name, intent_filter=None, limit=5):\n",
    "    \"\"\"\n",
    "    Finds the best full comments for a specific topic.\n",
    "    entity_name: The ID from your findings (e.g., 'Nemesis System')\n",
    "    intent_filter: Optional (e.g., 'WISHLIST', 'PAIN_POINT')\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Find the signals matching your criteria\n",
    "    matches = df_signals[df_signals['normalized_id'] == entity_name]\n",
    "    \n",
    "    if intent_filter:\n",
    "        matches = matches[matches['intent'] == intent_filter]\n",
    "        \n",
    "    # 2. Get the unique Comment IDs for these signals\n",
    "    target_ids = matches['comment_id'].unique()\n",
    "    \n",
    "    # 3. Fetch the FULL TEXT from the raw dataframe\n",
    "    quotes = df_raw[df_raw['comment_id'].isin(target_ids)].copy()\n",
    "    \n",
    "    if quotes.empty:\n",
    "        print(f\"‚ö†Ô∏è No quotes found for {entity_name} with filter {intent_filter}\")\n",
    "        return\n",
    "\n",
    "    # 4. Sort by Score (High visibility comments first)\n",
    "    quotes = quotes.sort_values(by='score', ascending=False).head(limit)\n",
    "    \n",
    "    print(f\"--- üó£Ô∏è VERBATIMS: {entity_name} ({intent_filter or 'ALL'}) ---\")\n",
    "    \n",
    "    for i, row in quotes.iterrows():\n",
    "        print(f\"\\n[{row['score']} pts] u/{row['author']} in r/{row['subreddit']}:\")\n",
    "        print(f\"\\\"{row['content']}\\\"\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d366e0c7-ba7c-4c80-b797-f96938b43696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 6: EXECUTE THE SEARCH\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"üîç DEBUG: Checking data availability...\")\n",
    "print(f\"   Signals Available: {len(df_signals)}\")\n",
    "print(f\"   Raw Comments Available: {len(df_raw)}\")\n",
    "\n",
    "print(\"\\nrunning search for 'Bugs'...\\n\")\n",
    "\n",
    "# 1. Search for \"Bugs\" (The #1 Pain Point)\n",
    "get_golden_quotes(\"Bugs\", intent_filter=\"PAIN_POINT\", limit=3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "\n",
    "# 2. Search for \"PC\" (The #1 Wishlist Item)\n",
    "get_golden_quotes(\"PC\", intent_filter=\"WISHLIST\", limit=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1af3b1e-9f17-46db-ace8-00dbf1da6da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 7: BATCH PROCESSOR (SCALING TO 20K)\n",
    "# ==============================================================================\n",
    "import gc # Garbage Collector to free memory\n",
    "import time\n",
    "\n",
    "# CONFIGURATION\n",
    "BATCH_SIZE = 5000\n",
    "TOTAL_TARGET = 20000\n",
    "BATCHES = TOTAL_TARGET // BATCH_SIZE\n",
    "\n",
    "all_signals = []\n",
    "print(f\"üöÄ STARTING BATCH RUN: {TOTAL_TARGET} comments ({BATCHES} batches)...\")\n",
    "print(\"   (This will take approx 10-15 minutes on a MacBook Air)\")\n",
    "\n",
    "for i in range(BATCHES):\n",
    "    offset = i * BATCH_SIZE\n",
    "    print(f\"\\nüîÑ BATCH {i+1}/{BATCHES} (Offset: {offset})\")\n",
    "    \n",
    "    # 1. Fetch Data (Using OFFSET to get new comments each time)\n",
    "    # Note: We order by 'id' to ensure we don't get duplicates, \n",
    "    # but for simplicity/speed here we use the same high-score logic with offset\n",
    "    batch_sql = f\"\"\"\n",
    "    SELECT \n",
    "        id as comment_id,\n",
    "        subreddit,\n",
    "        author,\n",
    "        content,\n",
    "        score,\n",
    "        created_at\n",
    "    FROM analysis_corpus\n",
    "    WHERE \n",
    "        subreddit IN ('lotr', 'tolkienfans', 'lotro', 'Rings_Of_Power', 'gaming', 'Eldenring', 'totalwar', 'baldursgate3')\n",
    "        AND user_authenticity_score(author) = 'HIGH'\n",
    "        AND content_tsv @@ to_tsquery('english', 'game | rpg | strategy | open <-> world | combat | mechanic | graphics | lore | story | quest | dev | studio')\n",
    "    ORDER BY score DESC\n",
    "    LIMIT {BATCH_SIZE} OFFSET {offset};\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"   ‚è≥ Fetching data...\")\n",
    "        df_batch = pd.read_sql(batch_sql, engine)\n",
    "        \n",
    "        if df_batch.empty:\n",
    "            print(\"   ‚ö†Ô∏è No more data found. Stopping early.\")\n",
    "            break\n",
    "            \n",
    "        print(f\"   üß† Analyzing {len(df_batch)} comments...\")\n",
    "        # Reuse your existing analyze function\n",
    "        df_batch_signals = analyze_signals(df_batch)\n",
    "        \n",
    "        # Append to master list\n",
    "        all_signals.append(df_batch_signals)\n",
    "        \n",
    "        # SAVE PROGRESS (Crucial for stability)\n",
    "        # We save a temporary CSV after each batch just in case\n",
    "        df_batch_signals.to_csv(f\"signals_batch_{i+1}.csv\", index=False)\n",
    "        print(f\"   üíæ Batch {i+1} saved locally.\")\n",
    "        \n",
    "        # CLEANUP (The MacBook Air Survival Tactic)\n",
    "        del df_batch\n",
    "        del df_batch_signals\n",
    "        gc.collect() # Force clear RAM\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error in Batch {i+1}: {e}\")\n",
    "        break\n",
    "\n",
    "# FINAL MERGE\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "if all_signals:\n",
    "    master_df = pd.concat(all_signals, ignore_index=True)\n",
    "    print(f\"‚úÖ RUN COMPLETE.\")\n",
    "    print(f\"üìä Total Verified Signals Extracted: {len(master_df)}\")\n",
    "    \n",
    "    # Save the Master File\n",
    "    master_df.to_csv(\"Embracer_Reddit_Signals_MASTER_v1.csv\", index=False)\n",
    "    print(\"üìÅ Saved to 'Embracer_Reddit_Signals_MASTER_v1.csv'\")\n",
    "    \n",
    "    # DISPLAY THE FINAL LEADERBOARD\n",
    "    print(\"\\nüëë FINAL WISHLIST LEADERBOARD:\")\n",
    "    print(master_df[master_df['intent'] == 'WISHLIST']['normalized_id'].value_counts().head(15))\n",
    "    \n",
    "    print(\"\\nüö® FINAL PAIN POINT LEADERBOARD:\")\n",
    "    print(master_df[master_df['intent'] == 'PAIN_POINT']['normalized_id'].value_counts().head(15))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bc5896-80d6-4b9a-8674-d2011dfab2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 8: VISUALIZATION GENERATOR (FOR PPT DECK)\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Load Master Data if not already in memory\n",
    "try:\n",
    "    df = pd.read_csv(\"Embracer_Reddit_Signals_MASTER_v1.csv\")\n",
    "except:\n",
    "    print(\"Master CSV not found. Please re-run the batch processor.\")\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "if not df.empty:\n",
    "    # SET STYLE\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    \n",
    "    # --- CHART 1: THE \"REAL\" WISHLIST (No Tech/PC Noise) ---\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Filter: Intent=WISHLIST, Exclude Category=TECH, Exclude \"PC\"\n",
    "    wishlist_data = df[\n",
    "        (df['intent'] == 'WISHLIST') & \n",
    "        (df['category'] != 'TECH') & \n",
    "        (df['normalized_id'] != 'PC')\n",
    "    ]['normalized_id'].value_counts().head(10)\n",
    "    \n",
    "    sns.barplot(x=wishlist_data.values, y=wishlist_data.index, palette=\"viridis\")\n",
    "    plt.title(\"What Fans Actually Want (Game Features Only)\", fontsize=16, weight='bold')\n",
    "    plt.xlabel(\"Verified Wishlist Mentions\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"Slide1_Real_Wishlist.png\", dpi=300)\n",
    "    print(\"‚úÖ Saved 'Slide1_Real_Wishlist.png'\")\n",
    "    \n",
    "    # --- CHART 2: THE \"PAIN LANDSCAPE\" (Tech vs Design) ---\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    pain_data = df[df['intent'] == 'PAIN_POINT']['normalized_id'].value_counts().head(10)\n",
    "    \n",
    "    # Color Code: RED for Tech (Bugs), ORANGE for Design (Lore/Mechanics)\n",
    "    colors = ['#d62728' if x in ['Bugs', 'Optimization', 'Console'] else '#ff7f0e' for x in pain_data.index]\n",
    "    \n",
    "    sns.barplot(x=pain_data.values, y=pain_data.index, palette=colors)\n",
    "    plt.title(\"The 'Trust Battery' Drain: Technical vs. Creative Complaints\", fontsize=16, weight='bold')\n",
    "    plt.xlabel(\"Verified Pain Point Mentions\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"Slide2_Pain_Points.png\", dpi=300)\n",
    "    print(\"‚úÖ Saved 'Slide2_Pain_Points.png'\")\n",
    "\n",
    "    # --- CHART 3: THE \"HIDDEN LOVE\" (Implicit Sentiment) ---\n",
    "    # What do they like even if they don't say \"I wish\"?\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    implicit_love = df[\n",
    "        (df['sentiment'] > 0.5) & \n",
    "        (df['category'].isin(['MECHANIC', 'AESTHETIC']))\n",
    "    ]['normalized_id'].value_counts().head(10)\n",
    "    \n",
    "    sns.barplot(x=implicit_love.values, y=implicit_love.index, palette=\"Blues_r\")\n",
    "    plt.title(\"Implicit Approval: Concepts with High Positive Sentiment\", fontsize=16, weight='bold')\n",
    "    plt.xlabel(\"Positive Mentions (>0.5 Sentiment)\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"Slide3_Implicit_Love.png\", dpi=300)\n",
    "    print(\"‚úÖ Saved 'Slide3_Implicit_Love.png'\")\n",
    "    \n",
    "else:\n",
    "    print(\"No data to plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63bacfe-0b5f-48f5-9787-73adafeb09a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
